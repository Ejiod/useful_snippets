import pyodbc
import pandas as pd
from datetime import datetime

# Initialize an empty DataFrame for logging
log_df = pd.DataFrame(columns=['timestamp', 'event'])

def log_event(event):
    global log_df
    log_df = log_df.append({
        'timestamp': datetime.now(),
        'event': event
    }, ignore_index=True)

def create_table_and_insert_data(df, table_name, connection):
    log_event(f'Starting creation of table {table_name}')
    # [... rest of your code ...]
    log_event(f'Table {table_name} created and data inserted')

def table_exists(table_name, connection):
    try:
        sql = f"SELECT * FROM {table_name} WHERE 1=0"
        connection.execute(sql)
        log_event(f'Table {table_name} exists')
        return True
    except:
        log_event(f'Table {table_name} does not exist')
        return False

def view_exists(view_name, connection):
    try:
        sql = f"SELECT * FROM {view_name} WHERE 1=0"
        connection.execute(sql)
        log_event(f'View {view_name} exists')
        return True
    except:
        log_event(f'View {view_name} does not exist')
        return False

def create_view(source_table, source_connection):
    log_event(f'Starting creation of view for table {source_table}')
    view_name = f"{source_table}_view"
    sql = f"CREATE VIEW {view_name} AS SELECT * FROM {source_table}"
    source_connection.execute(sql)
    source_connection.commit()
    log_event(f'View for table {source_table} created')

def delete_table(table_name, source_connection):
    log_event(f'Starting deletion of table {table_name}')
    sql = f"DROP TABLE {table_name}"
    source_connection.execute(sql)
    source_connection.commit()
    log_event(f'Table {table_name} deleted')

# Define the source and target table names
migration_df = pd.DataFrame({
    'source_db': ['TD500', 'TD500', 'TD500'],
    'target_db': ['TD800', 'TD800', 'TD800'],
    'source_table': ['tableA', 'tableB', 'tableC'],
    'target_table': ['tableA', 'tableB', 'tableC']
})

# Connect to the source and target servers
source_connection = pyodbc.connect('DSN=YourSourceDSN;UID=YourUsername;PWD=YourPassword')
target_connection = pyodbc.connect('DSN=YourTargetDSN;UID=YourUsername;PWD=YourPassword')

# Migrate each table
for _, row in migration_df.iterrows():
    log_event(f'Starting migration for source table {row["source_table"]}')
    df = pd.read_sql(f"SELECT * FROM {row['source_table']}", source_connection)
    create_table_and_insert_data(df, row['target_table'], target_connection)
    create_view(row['source_table'], source_connection)
    if table_exists(row['target_table'], target_connection) and view_exists(f"{row['source_table']}_view", source_connection):
        delete_table(row['source_table'], source_connection)
    log_event(f'Migration for source table {row["source_table"]} completed')

# Save the log DataFrame to a CSV file after all operations are done
log_df.to_csv('migration_log.csv', index=False)





--------UPDATE

import pyodbc
import pandas as pd

def create_table_and_insert_data(df, table_name, connection):
    # Start the CREATE MULTISET TABLE statement
    sql = f"CREATE MULTISET TABLE {table_name} ("
    # Add a column definition for each column in the DataFrame
    for col in df.columns:
        # Infer the Teradata data type from the DataFrame's data type
        if df[col].dtype == 'int64':
            teradata_type = 'INTEGER'
        elif df[col].dtype == 'float64':
            teradata_type = 'FLOAT'
        elif df[col].dtype == 'bool':
            teradata_type = 'BYTEINT'
        elif df[col].dtype == 'datetime64[ns]':
            teradata_type = 'TIMESTAMP'
        else:
            teradata_type = 'VARCHAR(255)'

        # Add the column definition to the SQL statement
        sql += f"\n    {col} {teradata_type},"

    # Remove the trailing comma and close the parenthesis
    sql = sql.rstrip(',') + "\n)"
    
    # Execute the CREATE TABLE statement
    with connection.cursor() as cur:
        cur.execute(sql)
    
    # Now, perform the bulk insert
    params = ','.join(['?'] * len(df.columns))
    for index, row in df.iterrows():
        with connection.cursor() as cur:
            cur.execute(f"INSERT INTO {table_name} VALUES ({params})", tuple(row))

def create_view(source_table, source_connection):
    view_name = f"{source_table}_view"
    sql = f"CREATE VIEW {view_name} AS SELECT * FROM {source_table}"
    
    with source_connection.cursor() as cur:
        cur.execute(sql)
        source_connection.commit()

def delete_table(table_name, source_connection):
    sql = f"DROP TABLE {table_name}"
    
    with source_connection.cursor() as cur:
        cur.execute(sql)
        source_connection.commit()

# Define the source and target table names
migration_df = pd.DataFrame({
    'source_db': ['TD500', 'TD500', 'TD500', 'TD500'],
    'source_table': ['XPA.Table1', 'XPA.Table2', 'XPA.Table3', 'XPA.Table4'],
    'target_db': ['TD800', 'TD800', 'TD800', 'TD800'],
    'target_table': ['DXV.Table1', 'DXV.Table2', 'DXV.Table3', 'DXV.Table4']
})

# Connect to the source and target servers
source_connection = pyodbc.connect('DSN=YourSourceDSN;UID=YourUsername;PWD=YourPassword')
target_connection = pyodbc.connect('DSN=YourTargetDSN;UID=YourUsername;PWD=YourPassword')

# Migrate each table and create views
for _, row in migration_df.iterrows():
    # Read the data from the source table
    df = pd.read_sql(f"SELECT * FROM {row['source_table']}", source_connection)
    
    # Create the target table and insert the data
    create_table_and_insert_data(df, row['target_table'], target_connection)
    
    # Create the view in the source database
    create_view(row['source_table'], source_connection)
    
    # Check if the target table and view exist and delete the source table
    if table_exists(row['target_table'], target_connection) and view_exists(f"{row['source_table']}_view", source_connection):
        delete_table(row['source_table'], source_connection)


---VIEW
import pyodbc

def create_view(source_table, source_connection):
    target_view = source_table.split('.')[-1] + '_view'
    view_name = f"PBA_RCB.{target_view}"
    sql = f"CREATE VIEW {view_name} AS (SELECT * FROM {source_table}@FS_TGB)"
    with source_connection.cursor() as cur:
        cur.execute(sql)
        source_connection.commit()

# Define the source table names
source_tables = ['GDB_VCA.Table1', 'GDB_VCA.Table2', 'GDB_VCA.Table3', ...]  # List all source tables here

# Connect to the source server
source_connection = pyodbc.connect('DSN=YourSourceDSN;UID=YourUsername;PWD=YourPassword')

# Create views based on the source tables
for source_table in source_tables:
    create_view(source_table, source_connection)



---BULK INSERT--

import pyodbc
import pandas as pd

def create_table_and_insert_data(df, table_name, connection):
    # Start the CREATE TABLE statement
    sql = f"CREATE TABLE {table_name} AS SELECT * FROM {df['source_db']}.{df['source_table']}"

    # Execute the CREATE TABLE statement
    with connection.cursor() as cur:
        cur.execute(sql)
    
    # Now, perform the bulk insert
    bulk_insert(table_name, df, connection)

def bulk_insert(table_name, df, connection):
    # Now, perform the bulk insert
    params = ','.join(['?'] * len(df.columns))
    values = [tuple(row) for _, row in df.iterrows()]

    with connection.cursor() as cur:
        cur.executemany(f"INSERT INTO {table_name} VALUES ({params})", values)

# Define the source and target table names
migration_df = pd.DataFrame({
    'source_db': ['TD800', 'TD800', 'TD800', 'TD800'],
    'source_table': ['DXV.Table1', 'DXV.Table2', 'DXV.Table3', 'DXV.Table4'],
    'target_db': ['TD700', 'TD700', 'TD700', 'TD700'],
    'target_table': ['XBA.GHA_Table1', 'XBA.GHA_Table2', 'XBA.GHA_Table3', 'XBA.GHA_Table4']
})

# Connect to the source and target servers
source_connection = pyodbc.connect('DSN=YourSourceDSN;UID=YourUsername;PWD=YourPassword')
target_connection = pyodbc.connect('DSN=YourTargetDSN;UID=YourUsername;PWD=YourPassword')

# Migrate each table
for _, row in migration_df.iterrows():
    df = pd.read_sql(f"SELECT * FROM {row['source_table']}", source_connection)
    create_table_and_insert_data(df, row['target_table'], target_connection)



--BULK COPY2--

import pandas as pd
import pyodbc

def create_table_and_insert_data(df, table_name, connection):
    # Start the CREATE TABLE statement
    sql = f"CREATE TABLE {table_name} AS SELECT * FROM {df['source_table']}"

    # Execute the CREATE TABLE statement
    with connection.cursor() as cur:
        cur.execute(sql)

def bulk_insert(table_name, df, connection):
    # Generate the parameter placeholders for the insert statement
    params = ','.join(['?'] * len(df.columns))

    # Prepare the insert statement
    insert_stmt = f"INSERT INTO {table_name} VALUES ({params})"

    # Prepare the data as a list of tuples
    values = [tuple(row) for _, row in df.iterrows()]

    # Execute the bulk insert
    with connection.cursor() as cur:
        cur.executemany(insert_stmt, values)

def table_exists(table_name, connection):
    try:
        sql = f"SELECT * FROM {table_name} WHERE 1=0"
        connection.execute(sql)
        return True
    except:
        return False

# Define the source and target table names
migration_df = pd.DataFrame({
    'source_db': ['TD800', 'TD800', 'TD800', 'TD800'],
    'source_table': ['DXV.Table1', 'DXV.Table2', 'DXV.Table3', 'DXV.Table4'],
    'target_db': ['TD700', 'TD700', 'TD700', 'TD700'],
    'target_table': ['XBA.GHA_Table1', 'XBA.GHA_Table2', 'XBA.GHA_Table3', 'XBA.GHA_Table4']
})

# Connect to the source and target databases
source_connection = pyodbc.connect('DSN=YourSourceDSN;UID=YourUsername;PWD=YourPassword')
target_connection = pyodbc.connect('DSN=YourTargetDSN;UID=YourUsername;PWD=YourPassword')

# Migrate each table
for _, row in migration_df.iterrows():
    if table_exists(row['target_table'], target_connection):
        continue  # Skip migration if target table already exists
    
    # Read the data from the source table
    df = pd.read_sql(f"SELECT * FROM {row['source_table']}", source_connection)

    # Create the target table
    create_table_and_insert_data(df, row['target_table'], target_connection)

    # Bulk insert the data into the target table
    bulk_insert(row['target_table'], df, target_connection)

print("Table migration completed.")






bulk3-----

import pandas as pd
import pyodbc

def create_table_and_insert_data(source_table, target_table, connection):
    # Start the CREATE TABLE statement
    sql = f"CREATE TABLE {target_table} AS (SELECT * FROM {source_table})"

    # Execute the CREATE TABLE statement
    with connection.cursor() as cursor:
        cursor.execute(sql)

def bulk_insert(table_name, df, connection):
    # Now, perform the bulk insert
    params = ','.join(['?'] * len(df.columns))
    values = [tuple(row) for _, row in df.iterrows()]

    with connection.cursor() as cursor:
        cursor.executemany(f"INSERT INTO {table_name} VALUES ({params})", values)

# Define the source and target table names
migration_df = pd.DataFrame({
    'source_db': ['TD800', 'TD800', 'TD800', 'TD800'],
    'source_table': ['DXV.Table1', 'DXV.Table2', 'DXV.Table3', 'DXV.Table4'],
    'target_db': ['TD700', 'TD700', 'TD700', 'TD700'],
    'target_table': ['XBA.GHA_Table1', 'XBA.GHA_Table2', 'XBA.GHA_Table3', 'XBA.GHA_Table4']
})

# Connect to the source and target databases
source_connection = pyodbc.connect('DSN=YourSourceDSN;UID=YourUsername;PWD=YourPassword')
target_connection = pyodbc.connect('DSN=YourTargetDSN;UID=YourUsername;PWD=YourPassword')

# Migrate each table
for _, row in migration_df.iterrows():
    # Create the target table
    create_table_and_insert_data(row['source_table'], row['target_table'], target_connection)

    # Read the data from the source table
    df = pd.read_sql(f"SELECT * FROM {row['source_table']}", source_connection)

    # Perform the bulk insert
    bulk_insert(row['target_table'], df, target_connection)
