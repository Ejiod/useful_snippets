import pyodbc
import pandas as pd

def create_table_and_insert_data(df, table_name, connection):
    # Start the CREATE MULTISET TABLE statement
    sql = f"CREATE MULTISET TABLE {table_name} ("
    # Add a column definition for each column in the DataFrame
    for col in df.columns:
        # Infer the Teradata data type from the DataFrame's data type
        if df[col].dtype == 'int64':
            teradata_type = 'INTEGER'
        elif df[col].dtype == 'float64':
            teradata_type = 'FLOAT'
        elif df[col].dtype == 'bool':
            teradata_type = 'BYTEINT'
        elif df[col].dtype == 'datetime64[ns]':
            teradata_type = 'TIMESTAMP'
        else:
            teradata_type = 'VARCHAR(255)'

        # Add the column definition to the SQL statement
        sql += f"\n    {col} {teradata_type},"

    # Remove the trailing comma and close the parenthesis
    sql = sql.rstrip(',') + "\n)"
    # Execute the CREATE TABLE statement
    connection.execute(sql)
    connection.commit()

    # Now, perform the bulk insert
    params = ','.join(['?'] * len(df.columns))
    for index, row in df.iterrows():
        connection.execute(f"INSERT INTO {table_name} VALUES ({params})", tuple(row))
    # Commit the transaction
    connection.commit()

def table_exists(table_name, connection):
    try:
        sql = f"SELECT * FROM {table_name} WHERE 1=0"
        connection.execute(sql)
        return True
    except:
        return False

def view_exists(view_name, connection):
    try:
        sql = f"SELECT * FROM {view_name} WHERE 1=0"
        connection.execute(sql)
        return True
    except:
        return False

def create_view(source_table, source_connection):
    view_name = f"{source_table}_view"
    sql = f"CREATE VIEW {view_name} AS SELECT * FROM {source_table}"
    source_connection.execute(sql)
    source_connection.commit()

def delete_table(table_name, source_connection):
    sql = f"DROP TABLE {table_name}"
    source_connection.execute(sql)
    source_connection.commit()

# Define the source and target table names
migration_df = pd.DataFrame({
    'source_db': ['TD500', 'TD500', 'TD500'],
    'target_db': ['TD800', 'TD800', 'TD800'],
    'source_table': ['tableA', 'tableB', 'tableC'],
    'target_table': ['tableA', 'tableB', 'tableC']
})

# Connect to the source and target servers
source_connection = pyodbc.connect('DSN=YourSourceDSN;UID=YourUsername;PWD=YourPassword')
target_connection = pyodbc.connect('DSN=YourTargetDSN;UID=YourUsername;PWD=YourPassword')

# Migrate each table
for _, row in migration_df.iterrows():
    df = pd.read_sql(f"SELECT * FROM {row['source_table']}", source_connection)
    create_table_and_insert_data(df, row['target_table'], target_connection)
    create_view(row['source_table'], source_connection)
    if table_exists(row['target_table'], target_connection) and view_exists(f"{row['source_table']}_view", source_connection):
        delete_table(row['source_table'], source_connection)






%macro runTestMacro;

    %do year = 2013 %to 2019;
        %do month = 5 %to 12;
            %let monthPadded = %sysfunc(putn(&month, z2.)); /* Pads the month with 0 if needed */
            %test(&year.&monthPadded, &year._&monthPadded);
        %end;
    %end;

%mend runTestMacro;

%runTestMacro;
