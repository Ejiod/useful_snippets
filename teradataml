import cx_Oracle
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# Oracle connection parameters
dsn = cx_Oracle.makedsn('host', 'port', 'sid/service')
connection = cx_Oracle.connect('username', 'password', dsn)

# SQL query to execute
query = "SELECT * FROM your_table"

# Create a cursor and execute the query
cursor = connection.cursor()
cursor.execute(query)

# Reduce chunk size if needed
chunk_size = 1000000  # Adjust based on initial testing
file_number = 1

# Fetch and write chunks
while True:
    rows = cursor.fetchmany(chunk_size)
    if not rows:
        break
    df = pd.DataFrame(rows, columns=[col[0] for col in cursor.description])
    
    # Define file path
    file_path = f'data_chunk_{file_number}.parquet'
    
    # Convert DataFrame to Parquet
    table = pa.Table.from_pandas(df, preserve_index=False)
    pq.write_table(table, file_path, compression='snappy')
    
    print(f'Wrote {file_path}')
    file_number += 1






import dask.dataframe as dd
from sqlalchemy import create_engine
from dask.distributed import Client

client = Client()  # starts a local Dask client

# Establish connection
engine = create_engine('oracle+cx_oracle://username:password@hostname:port/?service_name=service_name')

# Query
query = "SELECT * FROM your_table"

# Using Dask to handle larger-than-memory data by loading and processing in partitions
dask_df = dd.read_sql_table(table_name='your_table', uri=str(engine.url), index_col='id', # assuming 'id' is a unique column
                            npartitions=320)  # Adjust partitions depending on the size and number of files

# Writing each partition to Parquet
def write_parquet(df, file_number):
    file_path = f'data_chunk_{file_number}.parquet'
    df.to_parquet(file_path, engine='pyarrow')
    print(f'Wrote {file_path}')

# Apply function over each partition
dask_df.map_partitions(write_parquet, meta=(None, 'object')).compute()






import boto3

def upload_file_to_s3(file_path, bucket, object_name):
    """
    Upload a file directly to an S3 bucket

    :param file_path: Path to the local file to upload
    :param bucket: S3 bucket name
    :param object_name: Object name in S3 (including path)
    """
    s3_client = boto3.client('s3')
    try:
        s3_client.upload_file(file_path, bucket, object_name)
        print(f'Successfully uploaded {file_path} to s3://{bucket}/{object_name}')
    except Exception as e:
        print(f'Failed to upload {file_path}. Error: {str(e)}')

# Example usage
upload_file_to_s3('path/to/your/file.parquet', 'your-bucket-name', 'path/in/s3/file.parquet')
