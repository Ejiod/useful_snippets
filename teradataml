import cx_Oracle
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# Oracle connection parameters
dsn = cx_Oracle.makedsn('host', 'port', 'sid/service')
connection = cx_Oracle.connect('username', 'password', dsn)

# SQL query to execute
query = "SELECT * FROM your_table"

# Create a cursor and execute the query
cursor = connection.cursor()
cursor.execute(query)

# Reduce chunk size if needed
chunk_size = 1000000  # Adjust based on initial testing
file_number = 1

# Fetch and write chunks
while True:
    rows = cursor.fetchmany(chunk_size)
    if not rows:
        break
    df = pd.DataFrame(rows, columns=[col[0] for col in cursor.description])
    
    # Define file path
    file_path = f'data_chunk_{file_number}.parquet'
    
    # Convert DataFrame to Parquet
    table = pa.Table.from_pandas(df, preserve_index=False)
    pq.write_table(table, file_path, compression='snappy')
    
    print(f'Wrote {file_path}')
    file_number += 1






import dask.dataframe as dd
from sqlalchemy import create_engine
from dask.distributed import Client

client = Client()  # starts a local Dask client

# Establish connection
engine = create_engine('oracle+cx_oracle://username:password@hostname:port/?service_name=service_name')

# Query
query = "SELECT * FROM your_table"

# Using Dask to handle larger-than-memory data by loading and processing in partitions
dask_df = dd.read_sql_table(table_name='your_table', uri=str(engine.url), index_col='id', # assuming 'id' is a unique column
                            npartitions=320)  # Adjust partitions depending on the size and number of files

# Writing each partition to Parquet
def write_parquet(df, file_number):
    file_path = f'data_chunk_{file_number}.parquet'
    df.to_parquet(file_path, engine='pyarrow')
    print(f'Wrote {file_path}')

# Apply function over each partition
dask_df.map_partitions(write_parquet, meta=(None, 'object')).compute()






import os
import pandas as pd
import awswrangler as wr

def upload_parquet_files(folder_path, s3_bucket, s3_path):
    """
    Uploads all Parquet files from a local folder to S3.

    :param folder_path: Path to the folder containing Parquet files.
    :param s3_bucket: S3 bucket name.
    :param s3_path: Path inside the S3 bucket to store the files.
    """
    # Iterate over all files in the directory
    for filename in os.listdir(folder_path):
        if filename.endswith(".parquet"):
            file_path = os.path.join(folder_path, filename)
            # Create the full S3 path for each file
            full_s3_path = f"s3://{s3_bucket}/{s3_path}/{filename}"
            
            # Read the parquet file into a DataFrame
            df = pd.read_parquet(file_path)
            
            # Upload the DataFrame to S3 as a Parquet file
            wr.s3.to_parquet(
                df=df,
                path=full_s3_path,
                index=False,
                dataset=True  # Store as a dataset with metadata if required
            )
            print(f'Uploaded {filename} to {full_s3_path}')

# Example usage
upload_parquet_files('path/to/your/local/folder', 'your-bucket-name', 'path/in/s3')
