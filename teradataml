import os
import pandas as pd


# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.xlsx'):  # Check if the file is an Excel file
        file_path = os.path.join(folder_path, filename)
        xls = pd.ExcelFile(file_path)
        sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
        
        for sheet_name in sheet_names:
            df = pd.read_excel(file_path, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
            df['SheetName'] = sheet_name  # Add a new column for the sheet name
            all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed
combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)



(?:(?!not\s+sent|not\s+issued))

import glob
import pandas as pd

# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Use glob to get a list of Excel files in the folder
excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))

# Iterate through the Excel files
for excel_file in excel_files:
    xls = pd.ExcelFile(excel_file)
    sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
    
    for sheet_name in sheet_names:
        df = pd.read_excel(excel_file, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
        df['SheetName'] = sheet_name  # Add a new column for the sheet name
        all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed

combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)







import pandas as pd
from spellchecker import SpellChecker

# Load your data
file_path = './Downloads/test_rev.csv'  # Make sure to use your actual file path
data = pd.read_csv(file_path)

# Initialize the spell checker
spell = SpellChecker()

# Function to identify and correct misspelled words in a text
def correct_spelling(text):
    if pd.isna(text):
        return text  # Return as is if the text is NaN
    misspelled = spell.unknown(text.split())
    corrected_text = text
    for word in misspelled:
        # Get the most likely correction
        correction = spell.correction(word)
        # Ensure both word and correction are valid strings
        if isinstance(word, str) and isinstance(correction, str) and word and correction:
            # Replace the misspelled word with the correction
            corrected_text = corrected_text.replace(word, correction)
    return corrected_text

# Applying spell checking to the 'Topic' and 'Content' columns
data['Corrected Topic'] = data['Topic'].apply(correct_spelling)
data['Corrected Content'] = data['Content'].apply(correct_spelling)

# Save or display the results
data.to_csv('data_spell2.csv')  # Replace with your desired output file path





ERROR LOGS
import pandas as pd
from sqlalchemy import create_engine
import datetime
import getpass
import socket
import re


    except Exception as e:
        finish_time = datetime.now()
        error_message = str(e)
        session_match = re.search(r'\[Session (\d+)\]', error_message)
        error_code_match = re.search(r'\[Error (\d+)\]', error_message)

        session_id = session_match.group(1) if session_match else 'N/A'
        error_code = error_code_match.group(1) if error_code_match else 'N/A'
        error_detail = 'Table already exists' if 'already exists' in error_message else 'Other error'

        consolidated_metadata.append({
            'user': getpass.getuser(),
            'host': socket.gethostname(),
            'source_db': row['source_db'],
            'source_table': row['source_table'],
            'target_db': row['target_db'],
            'target_table': row['target_table'],
            'error_session': session_id,
            'error_code': error_code,
            'error_message': error_detail,
            'time_executed': start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'time_finished': finish_time.strftime('%Y-%m-%d %H:%M:%S'),
            'database_server': 'target_host'  # Adjust as needed
        })











----SF---SP
CREATE OR REPLACE FUNCTION process_info_udf(input_data VARIANT)
RETURNS TABLE (col1 VARIANT, col5 VARIANT, col7 VARIANT, matches_regex BOOLEAN)
LANGUAGE PYTHON
RUNTIME_VERSION = 3.8
PACKAGES = ('pandas',)
HANDLER = 'udf'
AS $$
import pandas as pd

def udf(input_data):
    # Convert input VARIANT data to Pandas DataFrame
    df = pd.DataFrame(input_data)

    # Define the regex pattern
    regex_pattern = (
     Regex pattern
    )

    # Apply regex to the 'info_column' (assuming 'info_column' is one of the columns in your data)
    df['matches_regex'] = df['info_column'].str.contains(regex_pattern, regex=True, na=False)

    # Select only the required columns
    df = df[['col1', 'col5', 'col7', 'matches_regex']]

    # Convert DataFrame to list of dictionaries
    return df.to_dict(orient='records')
$$;


SELECT *
FROM TABLE(process_info_udf(
    (SELECT OBJECT_CONSTRUCT(*) FROM GESR)
));



from snowflake.snowpark.functions import year, col

# Extract the year and get unique years
df_with_year = df.with_column("year", year(col("batch_month")))
unique_years = df_with_year.select("year").distinct().collect()

# Iterate over each year and write data for that year
for year_row in unique_years:
    year_value = year_row['year']
    batch_df = df_with_year.filter(col("year") == year_value)
    batch_df.write.mode("append").save_as_table("db2.schema2.new_table")





from snowflake.snowpark.functions import to_date, year, col

# Assuming df is your DataFrame and 'batch_month' is a string in 'YYYY-MM-DD' format
df_with_date = df.with_column("year", year(to_date(col("batch_month"), 'YYYY-MM-DD')))

unique_years = [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]

# Iterate over each year and write data for that year
for year_value in unique_years:
    batch_df = df_with_date.filter(col("year") == year_value)
    batch_df.write.mode("append").save_as_table("db2.schema2.new_table")

# Starting and ending year-month for the tables
start_year_month = (2004, 2)
end_year_month = (2024, 2)

# Initial table
initial_table = "SELECT * FROM b_gg_2004_01"

# Generating the table names and UNION ALL part of the query
tables = [initial_table]
for year in range(start_year_month[0], end_year_month[0] + 1):
    start_month = 1
    end_month = 12
    if year == start_year_month[0]:
        start_month = start_year_month[1]
    if year == end_year_month[0]:
        end_month = end_year_month[1]
    for month in range(start_month, end_month + 1):
        table_name = f"SELECT * FROM b_gg_{{year:04d}}_{{month:02d}}"
        tables.append(table_name)

# Joining the table queries with UNION ALL
sql_statement = "\nUNION ALL\n".join(tables)

print(sql_statement)




from sqlalchemy import create_engine
import pandas as pd

# Create an engine to your Teradata database
engine = create_engine('teradatasql://username:password@host/database')

# The SQL query you want to execute
sql_query = "SELECT * FROM your_large_table"

# Specify the chunk size
chunksize = 10000  # Adjust this based on your system's capabilities

# Variable to keep track of the chunk number
chunk_number = 0

# Use read_sql with chunksize to read the data in chunks
for chunk in pd.read_sql(sql=sql_query, con=engine, chunksize=chunksize):
    # Process each chunk here if needed

    # Save the chunk to a Parquet file
    # Construct a file name based on the chunk number
    file_name = f'output_chunk_{chunk_number}.parquet'
    
    # Save the DataFrame as a Parquet file
    chunk.to_parquet(file_name, index=False)
    
    # Increment the chunk number
    chunk_number += 1









from sqlalchemy import create_engine
import pandas as pd

# Create an engine to your Teradata database
engine = create_engine('teradatasql://username:password@host/database')

# The SQL query you want to execute
sql_query = "SELECT * FROM your_large_table"

# Specify the chunk size
chunksize = 10000  # Adjust this based on your system's capabilities

# Use read_sql with chunksize to read the data in chunks
for chunk in pd.read_sql(sql=sql_query, con=engine, chunksize=chunksize):
    # Process each chunk here
    # For example, you can perform data transformation, analysis, or directly save the chunk to a file
    # Example to save chunk to a CSV file, appending each chunk to the file:
    chunk.to_csv('output_file.csv', mode='a', index=False, header=not bool(chunk.first_valid_index()))

