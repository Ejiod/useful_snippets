import os
import pandas as pd


# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.xlsx'):  # Check if the file is an Excel file
        file_path = os.path.join(folder_path, filename)
        xls = pd.ExcelFile(file_path)
        sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
        
        for sheet_name in sheet_names:
            df = pd.read_excel(file_path, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
            df['SheetName'] = sheet_name  # Add a new column for the sheet name
            all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed
combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)



(?:(?!not\s+sent|not\s+issued))

import glob
import pandas as pd

# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Use glob to get a list of Excel files in the folder
excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))

# Iterate through the Excel files
for excel_file in excel_files:
    xls = pd.ExcelFile(excel_file)
    sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
    
    for sheet_name in sheet_names:
        df = pd.read_excel(excel_file, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
        df['SheetName'] = sheet_name  # Add a new column for the sheet name
        all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed

combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)







import pandas as pd
from spellchecker import SpellChecker

# Load your data
file_path = './Downloads/test_rev.csv'  # Make sure to use your actual file path
data = pd.read_csv(file_path)

# Initialize the spell checker
spell = SpellChecker()

# Function to identify and correct misspelled words in a text
def correct_spelling(text):
    if pd.isna(text):
        return text  # Return as is if the text is NaN
    misspelled = spell.unknown(text.split())
    corrected_text = text
    for word in misspelled:
        # Get the most likely correction
        correction = spell.correction(word)
        # Ensure both word and correction are valid strings
        if isinstance(word, str) and isinstance(correction, str) and word and correction:
            # Replace the misspelled word with the correction
            corrected_text = corrected_text.replace(word, correction)
    return corrected_text

# Applying spell checking to the 'Topic' and 'Content' columns
data['Corrected Topic'] = data['Topic'].apply(correct_spelling)
data['Corrected Content'] = data['Content'].apply(correct_spelling)

# Save or display the results
data.to_csv('data_spell2.csv')  # Replace with your desired output file path





ERROR LOGS
import pandas as pd
from sqlalchemy import create_engine
import datetime
import getpass
import socket
import re


    except Exception as e:
        finish_time = datetime.now()
        error_message = str(e)
        session_match = re.search(r'\[Session (\d+)\]', error_message)
        error_code_match = re.search(r'\[Error (\d+)\]', error_message)

        session_id = session_match.group(1) if session_match else 'N/A'
        error_code = error_code_match.group(1) if error_code_match else 'N/A'
        error_detail = 'Table already exists' if 'already exists' in error_message else 'Other error'

        consolidated_metadata.append({
            'user': getpass.getuser(),
            'host': socket.gethostname(),
            'source_db': row['source_db'],
            'source_table': row['source_table'],
            'target_db': row['target_db'],
            'target_table': row['target_table'],
            'error_session': session_id,
            'error_code': error_code,
            'error_message': error_detail,
            'time_executed': start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'time_finished': finish_time.strftime('%Y-%m-%d %H:%M:%S'),
            'database_server': 'target_host'  # Adjust as needed
        })











----SF---SP
CREATE OR REPLACE FUNCTION process_info_udf(input_data VARIANT)
RETURNS TABLE (col1 VARIANT, col5 VARIANT, col7 VARIANT, matches_regex BOOLEAN)
LANGUAGE PYTHON
RUNTIME_VERSION = 3.8
PACKAGES = ('pandas',)
HANDLER = 'udf'
AS $$
import pandas as pd

def udf(input_data):
    # Convert input VARIANT data to Pandas DataFrame
    df = pd.DataFrame(input_data)

    # Define the regex pattern
    regex_pattern = (
     Regex pattern
    )

    # Apply regex to the 'info_column' (assuming 'info_column' is one of the columns in your data)
    df['matches_regex'] = df['info_column'].str.contains(regex_pattern, regex=True, na=False)

    # Select only the required columns
    df = df[['col1', 'col5', 'col7', 'matches_regex']]

    # Convert DataFrame to list of dictionaries
    return df.to_dict(orient='records')
$$;


SELECT *
FROM TABLE(process_info_udf(
    (SELECT OBJECT_CONSTRUCT(*) FROM GESR)
));




CREATE OR REPLACE PROCEDURE process_text_with_regex(input_data VARIANT)
RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('pandas',)
HANDLER = 'run'
AS
$$
import pandas as pd

def run(ignored_session, input_data):
    # Convert input VARIANT data to Pandas DataFrame
    df = pd.DataFrame(input_data)

    # Define the regex pattern
    regex_pattern = (
    
    )

    # Apply regex to the specified column (assuming 'info_column' is the column name)
    df['matches_regex'] = df['info_column'].str.contains(regex_pattern, regex=True, na=False)

    # Perform any additional data manipulation as required

    # Convert the result to a desired format, e.g., JSON, CSV, etc.
    result = df.to_json(orient='records')

    return result
$$;


