import os
import pandas as pd


# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.xlsx'):  # Check if the file is an Excel file
        file_path = os.path.join(folder_path, filename)
        xls = pd.ExcelFile(file_path)
        sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
        
        for sheet_name in sheet_names:
            df = pd.read_excel(file_path, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
            df['SheetName'] = sheet_name  # Add a new column for the sheet name
            all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed
combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)





import glob
import pandas as pd

# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Use glob to get a list of Excel files in the folder
excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))

# Iterate through the Excel files
for excel_file in excel_files:
    xls = pd.ExcelFile(excel_file)
    sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
    
    for sheet_name in sheet_names:
        df = pd.read_excel(excel_file, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
        df['SheetName'] = sheet_name  # Add a new column for the sheet name
        all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed

combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)







--GLOB TO READ ALL PARQUET

import pandas as pd
import glob

# Define the path to your parquet files
parquet_files = glob.glob('path/to/parquet/files/*.parquet')

# Read and concatenate the parquet files into a single DataFrame
df_combined = pd.concat((pd.read_parquet(pf) for pf in parquet_files))

# Now df_combined holds all your data combined from the parquet files


import dask.dataframe as dd

# Read the parquet files into a Dask DataFrame
ddf = dd.read_parquet('path/to/parquet/files/*.parquet')

# Use Dask operations to manipulate the dataframe as needed
# For example, you can perform groupby, aggregations, etc.

# If you need to convert the Dask DataFrame to Pandas (be cautious with memory usage)
# df_combined = ddf.compute()






import dask.dataframe as dd

# Your keywords
keywords = ['GH', 'ANYWORD, 'BASE', 'CODE']

# Create a regex pattern to match any of the keywords
# This joins the keywords with the regex 'or' operator '|', and uses \b for word boundaries
pattern = '|'.join(r'\b{}\b'.format(keyword) for keyword in keywords)

# Read the parquet files into a Dask DataFrame
ddf = dd.read_parquet('path/to/parquet/files/*.parquet')

# Assuming 'text_column' is the name of the column you want to search within
# This will return a boolean series where the regex condition is True
matches = ddf['text_column'].str.contains(pattern, case=False, na=False)

# Filter the DataFrame based on the regex pattern
filtered_ddf = ddf[matches]

# Now filtered_ddf will contain only the rows where 'text_column' contains any of the keywords

REGEX2
import dask.dataframe as dd

# Your keywords
keywords = ['GH', 'ANYWORD, 'BASE', 'CODE']
pattern = '|'.join(r'\b{}\b'.format(keyword) for keyword in keywords)

# Read the parquet files into a Dask DataFrame
ddf = dd.read_parquet('path/to/parquet/files/*.parquet')

# Filter rows that contain any of the keywords
matches_keywords = ddf['text_column'].str.contains(pattern, case=False, na=False)

# Further filter to exclude rows with "not sent"
matches_not_sent = ~ddf['text_column'].str.contains("not sent", case=False, na=False)

# Apply both filters to the DataFrame
filtered_ddf = ddf[matches_keywords & matches_not_sent]

# Now filtered_ddf will contain only the rows where 'text_column' contains any of the keywords
# and does not contain the phrase "not sent"





-SPARK WAY

from pyspark.sql import SparkSession
from pyspark.sql import DataFrameWriter

# Initialize a Spark session
spark = SparkSession.builder \
    .appName("ParquetToSnowflake") \
    .getOrCreate()

# Read the Parquet files into a Spark DataFrame
df = spark.read.parquet("path/to/parquet/folder/")

# Optional: Process the data with Spark DataFrame transformations if needed

# Snowflake connection options
options = {
    "sfURL": "your_account.snowflakecomputing.com",
    "sfUser": "your_username",
    "sfPassword": "your_password",
    "sfDatabase": "your_database",
    "sfSchema": "your_schema",
    "sfWarehouse": "your_warehouse",
    "dbtable": "your_snowflake_table"
}

# Write the DataFrame to Snowflake
df.write \
    .format("net.snowflake.spark.snowflake") \
    .options(**options) \
    .option("dbtable", "your_snowflake_table") \
    .mode("append") \
    .save()

# Stop the Spark session
spark.stop()

