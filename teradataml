import pandas as pd
from sqlalchemy import create_engine

# Replace with your actual database connection details
engine = create_engine('postgresql://username:password@localhost:5432/mydatabase')

# SQL query to select data
query = "SELECT * FROM my_large_table"

# Define chunk size
chunk_size = 10000

# Process in chunks
for i, chunk in enumerate(pd.read_sql(query, engine, chunksize=chunk_size)):
    # Define file path
    file_path = f'data_chunk_{i}.parquet'
    
    # Save each chunk to a Parquet file
    chunk.to_parquet(file_path, engine='pyarrow', index=False)
    print(f'Written chunk {i} to {file_path}')

    # Construct the dictionary mapping column names to their SQL type declarations
    dtype_dict = {}
    for column in columns_info:
        sql_type = type_map.get(column['ColumnType'], lambda x: "UNKNOWN TYPE")(column)
        default = f" DEFAULT {column['DefaultValue']}" if column['DefaultValue'] else ''
        nullable = '' if column['Nullable'] == 'Y' else ' NOT NULL'
        dtype_dict[column['ColumnName']] = f"{sql_type}{default}{nullable}"
    
    return dtype_dict
