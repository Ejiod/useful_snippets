import cx_Oracle
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# Oracle connection parameters
dsn = cx_Oracle.makedsn('host', 'port', 'sid/service')
connection = cx_Oracle.connect('username', 'password', dsn)

# SQL query to execute
query = "SELECT * FROM your_table"

# Create a cursor and execute the query
cursor = connection.cursor()
cursor.execute(query)

# Reduce chunk size if needed
chunk_size = 1000000  # Adjust based on initial testing
file_number = 1

# Fetch and write chunks
while True:
    rows = cursor.fetchmany(chunk_size)
    if not rows:
        break
    df = pd.DataFrame(rows, columns=[col[0] for col in cursor.description])
    
    # Define file path
    file_path = f'data_chunk_{file_number}.parquet'
    
    # Convert DataFrame to Parquet
    table = pa.Table.from_pandas(df, preserve_index=False)
    pq.write_table(table, file_path, compression='snappy')
    
    print(f'Wrote {file_path}')
    file_number += 1






import dask.dataframe as dd
from sqlalchemy import create_engine
from dask.distributed import Client

client = Client()  # starts a local Dask client

# Establish connection
engine = create_engine('oracle+cx_oracle://username:password@hostname:port/?service_name=service_name')

# Query
query = "SELECT * FROM your_table"

# Using Dask to handle larger-than-memory data by loading and processing in partitions
dask_df = dd.read_sql_table(table_name='your_table', uri=str(engine.url), index_col='id', # assuming 'id' is a unique column
                            npartitions=320)  # Adjust partitions depending on the size and number of files

# Writing each partition to Parquet
def write_parquet(df, file_number):
    file_path = f'data_chunk_{file_number}.parquet'
    df.to_parquet(file_path, engine='pyarrow')
    print(f'Wrote {file_path}')

# Apply function over each partition
dask_df.map_partitions(write_parquet, meta=(None, 'object')).compute()







import polars as pl
from sqlalchemy import create_engine

# Establish connection
engine = create_engine('oracle+cx_oracle://username:password@hostname:port/?service_name=service_name')

# Query and read in chunks (assuming manual chunk handling)
query = "SELECT * FROM your_table"
chunks = pl.scan_sql(str(engine.url), query, return_table=True)  # Adjust based on actual capabilities

# Process and write chunks to Parquet
file_number = 1
for df in chunks:
    file_path = f'data_chunk_{file_number}.parquet'
    df.write_parquet(file_path)
    print(f'Wrote {file_path}')
    file_number += 1


# Clean up
cursor.close()
connection.close()
