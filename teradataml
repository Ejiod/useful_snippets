import teradataml as tdml


# Connect to database_name_1 with LDAP authentication
tdml.create_context(host=db1_host, username=db1_username, password=db1_password, database=db1_database,
                    method='ldap', ldap_host=ldap_host, ldap_username=ldap_username, ldap_password=ldap_password)

# Connect to database_name_2 with LDAP authentication
tdml.create_context(host=db2_host, username=db2_username, password=db2_password, database=db2_database,
                    method='ldap', ldap_host=ldap_host, ldap_username=ldap_username, ldap_password=ldap_password)

# Iterate over table mappings and copy tables
for mapping in table_mappings:
    # Read the source table
    source_table = tdml.DataFrame.from_table(f"{mapping['database_name_1']}.{mapping['source_table']}")

    # Copy the source table to the target table
    tdml.copy_to_sql(source_table, f"{mapping['database_name_2']}.{mapping['target_table']}", if_exists="replace")

# Close the connections
tdml.remove_context()




source_context = tdml.create_context(host=source_host, username=source_username, password=source_password,
                                    database=source_database, method='ldap', ldap_host=ldap_host,
                                    ldap_username=ldap_username, ldap_password=ldap_password)

# Convert DataFrames to Pandas DataFrames
pandas_data_frames = []
for table_name in table_list:
    df = tdml.DataFrame.from_table(table_name)
    pandas_data_frames.append(df.to_pandas())

# Close the source connection
tdml.remove_context(source_context)

# Establish connection to the target database with LDAP authentication
target_context = tdml.create_context(host=target_host, username=target_username, password=target_password,
                                    database=target_database, method='ldap', ldap_host=ldap_host,
                                    ldap_username=ldap_username, ldap_password=ldap_password)

# Iterate over the Pandas DataFrames and copy to target table
for i, df in enumerate(pandas_data_frames):
    table_name = table_list[i]
    tdml.copy_to_sql(tdml.DataFrame(df), table_name, if_exists="replace")

# Close the target connection
tdml.remove_context(target_context)





-----vERSION 2
pandas_data_frames = []
for table_name in table_list:
    table_name = ast.literal_eval(table_name)
    df = tdml.DataFrame.from_table(table_name)
    pandas_data_frames.append(df.to_pandas())

# Close the source connection
tdml.remove_context(source_context)

# Establish connection to the target database with LDAP authentication
target_context = tdml.create_context(host=target_host, username=target_username, password=target_password,
                                    database=target_database, method='ldap', ldap_host=ldap_host,
                                    ldap_username=ldap_username, ldap_password=ldap_password)

# Iterate over the Pandas DataFrames and copy to target table
for i, df in enumerate(pandas_data_frames):
    table_name = table_list[i]
    tdml.copy_to_sql(tdml.DataFrame(df), table_name, if_exists="replace")



--RESET INDEX
pandas_data_frames = []
for table_name in table_list:
    df = tdml.DataFrame.from_table(table_name)
    df = df.to_pandas()
    
    # Reset index for each level of the multi-index
    for level in range(df.columns.nlevels):
        df = df.reset_index(level=level)
    
    pandas_data_frames.append(df)


pandas_data_frames = []
for table_name in table_list:
    df = tdml.DataFrame.from_table(table_name)
    column_names = df.columns.tolist()  # Get the column names
    selected_df = df[column_names]  # Select all columns explicitly
    pandas_data_frames.append(selected_df.to_pandas())



















-----PYODBC


import pyodbc
import pandas as pd
from teradataml import create_context, copy_to_sql, DataFrame

# Replace these with your source database details
source_driver = "{your_driver}"
source_server = "your_server"
source_database = "your_database"
source_username = "your_username"
source_password = "your_password"

# Replace these with your target Teradata Vantage details
target_host = "target_host"
target_username = "target_username"
target_password = "target_password"

# Create a new connection to the source database
source_conn = pyodbc.connect(driver=source_driver, server=source_server, database=source_database, uid=source_username, pwd=source_password)

# A dictionary to hold the DataFrames for each source table
source_dfs = {}

# Iterate over each row in the DataFrame
for index, row in tables_df.iterrows():
    source_db = row['database_name_1']
    source_table = row['source_table']

    # Read the source table into a DataFrame and store it in the dictionary
    source_dfs[source_table] = pd.read_sql(f"SELECT * FROM {source_db}.{source_table}", source_conn)

# Close the source connection
source_conn.close()

# Create a new context for the target Teradata Vantage
create_context(host=target_host, username=target_username, password=target_password)

# Copy each DataFrame to the corresponding target table
for index, row in tables_df.iterrows():
    source_table = row['source_table']
    target_db = row['database_name_2']
    target_table = row['target_table'] if pd.notna(row['target_table']) else source_table

    # Get the DataFrame for this source table
    source_df = source_dfs[source_table]

    # Copy the DataFrame to the target table
    copy_to_sql(df=source_df, table_name=f"{target_db}.{target_table}", if_exists="replace")



GET COUNT

from teradataml import DataFrame

def count_rows(tables):
    counts = {}
    for table in tables:
        df = DataFrame.from_table(table_name=table)
        counts[table] = df.shape[0]
    return counts
tables = ["A", "B", "C"]
print(count_rows(tables))


import pandas as pd
from teradataml import DataFrame

def count_rows_and_columns(tables):
    counts = {}
    for table in tables:
        df = DataFrame.from_table(table_name=table)
        row_count = df.shape[0]
        column_count = len(df.columns)
        counts[table] = {'Row_Count': row_count, 'Column_Count': column_count}

    # convert dictionary to DataFrame
    df_counts = pd.DataFrame.from_dict(counts, orient='index')

    return df_counts




----CHUNK SIZE


chunksize = 10 ** 6  # size of chunks
chunks = []

for chunk in pd.read_sql('SELECT * FROM large_table', conn, chunksize=chunksize):
    # Process each chunk here
    chunk_processed = process(chunk)
    chunks.append(chunk_processed)

# Concatenate all chunks into one DataFrame
df = pd.concat(chunks, axis=0)



--VIEW 
for index, row in tables_df.iterrows():
    source_db = row['database_name_1']
    source_table = row['source_table']

    # Define the view name (using the table name for simplicity)
    view_name = f"{source_table}_view"

    # Define the view query
    view_query = f"SELECT * FROM {source_db}.{source_table}"

    # Execute the query to create the view
    source_conn.execute(f'CREATE VIEW {view_name} AS {view_query}')





S
1. **Prepare a list of tables to be archived.**
    - The list should be in a format that can be read into a pandas DataFrame, such as an Excel or CSV file. This list will be used to identify the tables that need to be archived.
    
    - The format of the list should be as follows:

    | database_name_1 | source_table | database_name_2 | target_table |
    |-----------------|--------------|-----------------|--------------|
    | source_db1      | table1       | target_db1      | table1       |
    | source_db2      | table2       | target_db2      | table2       |
    | ...             | ...          | ...             | ...          |

    Here, `database_name_1` and `source_table` are the database and table names in the source database, and `database_name_2` and `target_table` are the database and table names in the target database. If the target table name is not provided, the source table name will be used.

    - When creating this list, please ensure that all table names and database names are spelled correctly and match exactly with their names in the database. An incorrect or misspelled name can cause the archiving process to fail.

    - Remember to include all the tables that need to be archived. If a table is not included in the list, it will not be archived. Conversely, if a table is included in the list, the archiving process will attempt to archive it, so please double-check the list to ensure it only contains the tables that need to be archived.

    - The order of the tables in the list does not matter for the archiving process, so you can list the tables in any order that is convenient for you.

    - Please save and send me the final list when it's ready, so we can proceed with the archiving process.

This list will be an essential part of our archiving process, and we need to ensure it is accurate and complete. Once the list is prepared, we can use it to read the tables into pandas DataFrames, store them, and then copy them to the target database.

Please feel free to reach out if you have any questions or need further clarification on any of the steps.





