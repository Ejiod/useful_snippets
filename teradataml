import pandas as pd
from sqlalchemy import create_engine

# Replace with your actual database connection details
engine = create_engine('postgresql://username:password@localhost:5432/mydatabase')

# SQL query to select data
query = "SELECT * FROM my_large_table"

# Define chunk size
chunk_size = 10000

# Process in chunks
for i, chunk in enumerate(pd.read_sql(query, engine, chunksize=chunk_size)):
    # Define file path
    file_path = f'data_chunk_{i}.parquet'
    
    # Save each chunk to a Parquet file
    chunk.to_parquet(file_path, engine='pyarrow', index=False)
    print(f'Written chunk {i} to {file_path}')
import pandas as pd
from snowflake.connector.pandas_tools import write_pandas
import snowflake.connector

# Define your Snowflake connection parameters
conn = snowflake.connector.connect(
    user='YOUR_USERNAME',
    password='YOUR_PASSWORD',
    account='YOUR_ACCOUNT',
    warehouse='YOUR_WAREHOUSE',
    database='YOUR_DATABASE',
    schema='YOUR_SCHEMA'
)

# Define the size of each chunk
chunk_size = 100000

# Create a reader object for the CSV
reader = pd.read_csv('path_to_your_file.csv', chunksize=chunk_size)

# Iterate over each chunk
for i, chunk in enumerate(reader):
    # Optional: Define the table name dynamically or statically
    table_name = f"your_table_name_chunk_{i}"
    
    # Write the DataFrame chunk to Snowflake
    write_pandas(conn, chunk, table_name)
    
    # Optionally, you can also output to confirm each chunk is processed
    print(f"Chunk {i} written to table {table_name}")

# Close the connection to Snowflake
conn.close()
