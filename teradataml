import os
import pandas as pd


# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.xlsx'):  # Check if the file is an Excel file
        file_path = os.path.join(folder_path, filename)
        xls = pd.ExcelFile(file_path)
        sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
        
        for sheet_name in sheet_names:
            df = pd.read_excel(file_path, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
            df['SheetName'] = sheet_name  # Add a new column for the sheet name
            all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed
combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)





import glob
import pandas as pd

# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Use glob to get a list of Excel files in the folder
excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))

# Iterate through the Excel files
for excel_file in excel_files:
    xls = pd.ExcelFile(excel_file)
    sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
    
    for sheet_name in sheet_names:
        df = pd.read_excel(excel_file, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
        df['SheetName'] = sheet_name  # Add a new column for the sheet name
        all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed

combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)









import dask.dataframe as dd
import pandas as pd
import cx_Oracle

# Setup the connection as before
dsn_tns = cx_Oracle.makedsn('192.168.1.1', '1521', service_name='my_service_name')
conn = cx_Oracle.connect('user', 'passwd', dsn_tns)

# Define a SQL query
query = "SELECT * FROM your_table"

# Use Pandas to read the SQL query in chunks
chunk_size = 50000  # Adjust chunk size based on your memory and dataset size
dfs = []  # This list will hold each chunk as a Pandas DataFrame

for chunk in pd.read_sql(query, conn, chunksize=chunk_size):
    # Convert each Pandas DataFrame into a Dask DataFrame and append to the list
    ddf = dd.from_pandas(chunk, npartitions=1)
    dfs.append(ddf)

# Concatenate all Dask DataFrames into one
dask_df = dd.concat(dfs)

# Write the Dask DataFrame to Parquet
dask_df.to_parquet('output_directory/my_data.parquet', write_index=False)

# No need to close the connection when using 'with' context manager
conn.close()

