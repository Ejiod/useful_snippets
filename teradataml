import os
import pandas as pd


# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.xlsx'):  # Check if the file is an Excel file
        file_path = os.path.join(folder_path, filename)
        xls = pd.ExcelFile(file_path)
        sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
        
        for sheet_name in sheet_names:
            df = pd.read_excel(file_path, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
            df['SheetName'] = sheet_name  # Add a new column for the sheet name
            all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed
combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)



(?:(?!not\s+sent|not\s+issued))

import glob
import pandas as pd

# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Use glob to get a list of Excel files in the folder
excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))

# Iterate through the Excel files
for excel_file in excel_files:
    xls = pd.ExcelFile(excel_file)
    sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
    
    for sheet_name in sheet_names:
        df = pd.read_excel(excel_file, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
        df['SheetName'] = sheet_name  # Add a new column for the sheet name
        all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed

combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)







import pandas as pd
from spellchecker import SpellChecker

# Load your data
file_path = './Downloads/test_rev.csv'  # Make sure to use your actual file path
data = pd.read_csv(file_path)

# Initialize the spell checker
spell = SpellChecker()

# Function to identify and correct misspelled words in a text
def correct_spelling(text):
    if pd.isna(text):
        return text  # Return as is if the text is NaN
    misspelled = spell.unknown(text.split())
    corrected_text = text
    for word in misspelled:
        # Get the most likely correction
        correction = spell.correction(word)
        # Ensure both word and correction are valid strings
        if isinstance(word, str) and isinstance(correction, str) and word and correction:
            # Replace the misspelled word with the correction
            corrected_text = corrected_text.replace(word, correction)
    return corrected_text

# Applying spell checking to the 'Topic' and 'Content' columns
data['Corrected Topic'] = data['Topic'].apply(correct_spelling)
data['Corrected Content'] = data['Content'].apply(correct_spelling)

# Save or display the results
data.to_csv('data_spell2.csv')  # Replace with your desired output file path





ERROR LOGS
import pandas as pd
from sqlalchemy import create_engine
import datetime
import getpass
import socket
import re


    except Exception as e:
        finish_time = datetime.now()
        error_message = str(e)
        session_match = re.search(r'\[Session (\d+)\]', error_message)
        error_code_match = re.search(r'\[Error (\d+)\]', error_message)

        session_id = session_match.group(1) if session_match else 'N/A'
        error_code = error_code_match.group(1) if error_code_match else 'N/A'
        error_detail = 'Table already exists' if 'already exists' in error_message else 'Other error'

        consolidated_metadata.append({
            'user': getpass.getuser(),
            'host': socket.gethostname(),
            'source_db': row['source_db'],
            'source_table': row['source_table'],
            'target_db': row['target_db'],
            'target_table': row['target_table'],
            'error_session': session_id,
            'error_code': error_code,
            'error_message': error_detail,
            'time_executed': start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'time_finished': finish_time.strftime('%Y-%m-%d %H:%M:%S'),
            'database_server': 'target_host'  # Adjust as needed
        })











----SF---SP
CREATE OR REPLACE FUNCTION process_info_udf(input_data VARIANT)
RETURNS TABLE (col1 VARIANT, col5 VARIANT, col7 VARIANT, matches_regex BOOLEAN)
LANGUAGE PYTHON
RUNTIME_VERSION = 3.8
PACKAGES = ('pandas',)
HANDLER = 'udf'
AS $$
import pandas as pd

def udf(input_data):
    # Convert input VARIANT data to Pandas DataFrame
    df = pd.DataFrame(input_data)

    # Define the regex pattern
    regex_pattern = (
     Regex pattern
    )

    # Apply regex to the 'info_column' (assuming 'info_column' is one of the columns in your data)
    df['matches_regex'] = df['info_column'].str.contains(regex_pattern, regex=True, na=False)

    # Select only the required columns
    df = df[['col1', 'col5', 'col7', 'matches_regex']]

    # Convert DataFrame to list of dictionaries
    return df.to_dict(orient='records')
$$;


SELECT *
FROM TABLE(process_info_udf(
    (SELECT OBJECT_CONSTRUCT(*) FROM GESR)
));



from snowflake.snowpark.functions import year, col

# Extract the year and get unique years
df_with_year = df.with_column("year", year(col("batch_month")))
unique_years = df_with_year.select("year").distinct().collect()

# Iterate over each year and write data for that year
for year_row in unique_years:
    year_value = year_row['year']
    batch_df = df_with_year.filter(col("year") == year_value)
    batch_df.write.mode("append").save_as_table("db2.schema2.new_table")





from snowflake.snowpark.functions import to_date, year, col

# Assuming df is your DataFrame and 'batch_month' is a string in 'YYYY-MM-DD' format
df_with_date = df.with_column("year", year(to_date(col("batch_month"), 'YYYY-MM-DD')))

unique_years = [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]

# Iterate over each year and write data for that year
for year_value in unique_years:
    batch_df = df_with_date.filter(col("year") == year_value)
    batch_df.write.mode("append").save_as_table("db2.schema2.new_table")

# Starting and ending year-month for the tables
start_year_month = (2004, 2)
end_year_month = (2024, 2)

# Initial table
initial_table = "SELECT * FROM b_gg_2004_01"

# Generating the table names and UNION ALL part of the query
tables = [initial_table]
for year in range(start_year_month[0], end_year_month[0] + 1):
    start_month = 1
    end_month = 12
    if year == start_year_month[0]:
        start_month = start_year_month[1]
    if year == end_year_month[0]:
        end_month = end_year_month[1]
    for month in range(start_month, end_month + 1):
        table_name = f"SELECT * FROM b_gg_{{year:04d}}_{{month:02d}}"
        tables.append(table_name)

# Joining the table queries with UNION ALL
sql_statement = "\nUNION ALL\n".join(tables)

print(sql_statement)




from sqlalchemy import create_engine
import pandas as pd

# Create an engine to your Teradata database
engine = create_engine('teradatasql://username:password@host/database')

# The SQL query you want to execute
sql_query = "SELECT * FROM your_large_table"

# Specify the chunk size
chunksize = 10000  # Adjust this based on your system's capabilities

# Variable to keep track of the chunk number
chunk_number = 0

# Use read_sql with chunksize to read the data in chunks
for chunk in pd.read_sql(sql=sql_query, con=engine, chunksize=chunksize):
    # Process each chunk here if needed

    # Save the chunk to a Parquet file
    # Construct a file name based on the chunk number
    file_name = f'output_chunk_{chunk_number}.parquet'
    
    # Save the DataFrame as a Parquet file
    chunk.to_parquet(file_name, index=False)
    
    # Increment the chunk number
    chunk_number += 1









from sqlalchemy import create_engine
import pandas as pd

# Create an engine to your Teradata database
engine = create_engine('teradatasql://username:password@host/database')

# The SQL query you want to execute
sql_query = "SELECT * FROM your_large_table"

# Specify the chunk size
chunksize = 10000  # Adjust this based on your system's capabilities

# Use read_sql with chunksize to read the data in chunks
for chunk in pd.read_sql(sql=sql_query, con=engine, chunksize=chunksize):
    # Process each chunk here
    # For example, you can perform data transformation, analysis, or directly save the chunk to a file
    # Example to save chunk to a CSV file, appending each chunk to the file:
    chunk.to_csv('output_file.csv', mode='a', index=False, header=not bool(chunk.first_valid_index()))







Pyathena


from pyathena import connect
import pandas as pd
import re

# Connect to Athena
conn = connect(s3_staging_dir='s3://YOUR_S3_BUCKET_NAME/path/to/',
               region_name='YOUR_AWS_REGION')

# Specify the database
database_name = 'your_database_name'

# Query to list tables in the specified database
list_tables_query = f"SHOW TABLES IN {database_name}"
tables_df = pd.read_sql(list_tables_query, conn)

# Define a regex pattern to match tables starting with B_account_ followed by a 4-digit year and a 2-digit month
pattern = re.compile(r'^B_account_\d{4}_\d{2}$')

# Filter tables using the regex pattern
filtered_tables = [table for table in tables_df['tab_name'] if pattern.match(table)]

# Loop through the filtered tables
for table_name in filtered_tables:
    print(f"Processing table: {table_name}")
    # Example query to select top 10 rows from each table
    query = f"SELECT * FROM {database_name}.{table_name} LIMIT 10"
    df = pd.read_sql(query, conn)
    # Here you can process the dataframe as needed
    print(df.head())  # Display the first few rows of the dataframe


from pyathena import connect
import pandas as pd
import re

# Connect to Athena using the appropriate S3 staging directory and AWS region
conn = connect(s3_staging_dir='s3://your_s3_bucket/path/to/',
               region_name='your_aws_region')

# Specify the Athena database
database_name = 'your_database_name'

# Query to fetch all table names in the specified database
list_tables_query = f"SHOW TABLES IN {database_name}"
tables_df = pd.read_sql(list_tables_query, conn)

# Regular expression to match tables starting with 'B_account_' followed by a 4-digit year and a 2-digit month
regex_pattern = r'^B_account_\d{4}_\d{2}$'

# Filter the DataFrame for table names that match the regex pattern
filtered_tables = tables_df[tables_df['table_name'].str.match(regex_pattern)]

# Loop through each filtered table name
for index, row in filtered_tables.iterrows():
    table_name = row['table_name']
    print(f"Processing table: {table_name}")
    # Here, you can place your query logic, for example, a simple SELECT query
    query = f"SELECT * FROM {table_name} LIMIT 10"
    df = pd.read_sql(query, conn)
    # Display or process the query results
    print(df)






from pyathena import connect
import pandas as pd
import re
from datetime import datetime

# Function to generate a list of expected table name suffixes for the date range
def generate_expected_suffixes(start_year, start_month, end_year, end_month):
    start_date = datetime(start_year, start_month, 1)
    end_date = datetime(end_year, end_month, 1)
    current_date = start_date
    
    expected_suffixes = []
    
    while current_date <= end_date:
        expected_suffixes.append(current_date.strftime('%Y_%m'))
        current_date = datetime(current_date.year + (current_date.month // 12), ((current_date.month % 12) + 1), 1)
        
    return expected_suffixes

# Connect to Athena using the appropriate S3 staging directory and AWS region
conn = connect(s3_staging_dir='s3://your_s3_bucket/path/to/',
               region_name='your_aws_region')

# Specify the Athena database
database_name = 'your_database_name'

# Query to fetch all table names in the specified database
list_tables_query = f"SHOW TABLES IN {database_name}"
tables_df = pd.read_sql(list_tables_query, conn)

# Regular expression to match tables starting with 'B_account_' followed by a 4-digit year and a 2-digit month
regex_pattern = r'^B_account_(\d{4})_(\d{2})$'

# Generate expected table name suffixes for the date range
expected_suffixes = generate_expected_suffixes(2000, 1, 2024, 1)

# Filter and verify tables cover the specified date range
covered_suffixes = []

for table in tables_df['tab_name']:
    match = re.match(regex_pattern, table)
    if match:
        suffix = f"{match.group(1)}_{match.group(2)}"
        if suffix in expected_suffixes:
            covered_suffixes.append(suffix)
            
missing_suffixes = set(expected_suffixes) - set(covered_suffixes)

if not missing_suffixes:
    print("All expected tables from January 2000 to January 2024 are covered.")
else:
    print("Missing tables for the following months:", missing_suffixes)






from pyathena import connect
import pandas as pd
import re

# Connect to Athena
conn = connect(s3_staging_dir='s3://your_s3_bucket/path/to/',
               region_name='your_aws_region')

# Specify the database
database_name = 'your_database_name'

# Query to list tables in the specified database
list_tables_query = f"SHOW TABLES IN {database_name}"
tables_df = pd.read_sql(list_tables_query, conn)

# Define a regex pattern to match tables in the format 'B_account_YYYY_MM'
pattern = re.compile(r'^B_account_(\d{4})_(\d{2})$')

# Filter tables using the regex pattern and check if they cover the desired date range
expected_range = pd.period_range(start='2000-01', end='2024-01', freq='M').strftime('%Y_%m')
covered_tables = []

for table in tables_df['tab_name']:
    match = pattern.match(table)
    if match:
        year_month = f"{match.group(1)}_{match.group(2)}"
        if year_month in expected_range:
            covered_tables.append(year_month)

# Identify any gaps in the coverage
covered_range = pd.Series(covered_tables).unique()
missing_range = list(set(expected_range) - set(covered_range))

if not missing_range:
    print("All months from January 2000 to January 2024 are covered.")
else:
    print(f"Missing table coverage for the following months: {sorted(missing_range)}")










from pyathena import connect
import polars as pl
import re

# Connect to Athena using the appropriate S3 staging directory and AWS region
conn = connect(s3_staging_dir='s3://your_s3_bucket/path/to/',
               region_name='your_aws_region')

# Specify the Athena database
database_name = 'your_database_name'

# Query to fetch all table names in the specified database
list_tables_query = f"SHOW TABLES IN {database_name}"

# Fetch tables using PyAthena and convert the results to a Polars DataFrame
tables_df = conn.cursor().execute(list_tables_query).as_pandas()
tables_pl = pl.from_pandas(tables_df)

# Regular expression to match tables starting with 'B_account_' followed by a 4-digit year and a 2-digit month
regex_pattern = r'^B_account_\d{4}_\d{2}$'

# Filter the DataFrame for table names that match the regex pattern
# Polars doesn't support regex filtering natively like pandas, so we convert to a list, filter, then recreate a DataFrame
filtered_table_names = [name for name in tables_pl['tab_name'].to_list() if re.match(regex_pattern, name)]
filtered_tables_pl = pl.DataFrame({'table_name': filtered_table_names})

# Loop through each filtered table name
for row in filtered_tables_pl.iterrows():
    table_name = row[1]['table_name']
    print(f"Processing table: {table_name}")
    # Here, you can place your query logic, for example, a simple SELECT query
    query = f"SELECT * FROM {table_name} LIMIT 10"
    df = conn.cursor().execute(query).as_pandas()  # Fetch data with PyAthena and convert to pandas DataFrame
    df_pl = pl.from_pandas(df)  # Optionally convert pandas DataFrame to Polars DataFrame if needed
    # Display or process the query results
    print(df_pl)





from pyathena import connect
import polars as pl
import re

# Connect to Athena using the appropriate S3 staging directory and AWS region
conn = connect(s3_staging_dir='s3://your_s3_bucket/path/to/',
               region_name='your_aws_region')

# Specify the Athena database
database_name = 'your_database_name'

# Execute the query to fetch all table names in the specified database
list_tables_query = f"SHOW TABLES IN {database_name}"
tables_df = conn.cursor().execute(list_tables_query).as_pandas()

# Convert the pandas DataFrame to a Polars DataFrame
tables_pl = pl.from_pandas(tables_df)

# Define the regex pattern
regex_pattern = r'^B_account_\d{4}_\d{2}$'

# Filter the DataFrame for table names that match the regex pattern
# Since Polars does not support regex filtering directly, we perform the operation outside of the DataFrame
filtered_table_names = [name for name in tables_pl['tab_name'].to_list() if re.match(regex_pattern, name)]

# For demonstration, let's assume we fetch and concatenate data from all filtered tables into a single Polars DataFrame
# Note: Adjust the query according to your actual data structure and needs
all_data_frames = []

for table_name in filtered_table_names:
    query = f"SELECT * FROM {database_name}.{table_name} LIMIT 10"  # Adjust the LIMIT as needed
    df = conn.cursor().execute(query).as_pandas()  # Fetch data with PyAthena and convert to pandas DataFrame
    df_pl = pl.from_pandas(df)  # Convert pandas DataFrame to Polars DataFrame
    all_data_frames.append(df_pl)

# Concatenate all fetched data into a single Polars DataFrame
final_df = pl.concat(all_data_frames)

# Save the final Polars DataFrame to a Parquet file
final_df.write_parquet('filtered_data.parquet')




from pyathena import connect
import pandas as pd

# Connect to Athena using the appropriate S3 staging directory and AWS region
conn = connect(s3_staging_dir='s3://your_s3_bucket/path/to/',
               region_name='your_aws_region')

# Specify the Athena database
database_name = 'your_database_name'

# Query to fetch all table names in the specified database
list_tables_query = f"SHOW TABLES IN {database_name}"
tables_df = pd.read_sql(list_tables_query, conn)

# Regular expression pattern to filter tables
regex_pattern = r'^B_account_\d{4}_\d{2}$'

# Filter table names based on the regex pattern
filtered_table_names = tables_df[tables_df['tab_name'].str.match(regex_pattern)]['tab_name']

# Loop through each filtered table name and export data to a Parquet file
for table_name in filtered_table_names:
    print(f"Processing and exporting table: {table_name}")
    query = f"SELECT * FROM {database_name}.{table_name}"
    df = pd.read_sql(query, conn)
    
    # Define the Parquet file name
    parquet_file_name = f"{table_name}.parquet"
    
    # Save the DataFrame as a Parquet file
    df.to_parquet(parquet_file_name, engine='pyarrow')

    print(f"Exported {table_name} to {parquet_file_name}")











# Start building the UNION ALL query
union_queries = []

for table_name in filtered_table_names:
    union_query = f"SELECT * FROM {database_name}.{table_name}"
    union_queries.append(union_query)

# Combine all union queries into a single query string with UNION ALL between them
combined_union_query = " UNION ALL ".join(union_queries)

# SQL to create a new Athena table with the combined data
new_table_name = "combined_table"
create_table_query = f"""
CREATE TABLE {database_name}.{new_table_name}
WITH (
    format = 'Parquet',
    external_location = 's3://your_s3_bucket/path/to/{new_table_name}/'
) AS
{combined_union_query}
"""

# Execute the query to create the new table
print(f"Creating new table: {new_table_name} with combined data")
conn.cursor().execute(create_table_query)
print(f"New table {new_table_name} created successfully.")







from pyathena import connect
import pandas as pd

# Initialize an empty DataFrame for aggregating data
combined_df = pd.DataFrame()

# Loop through each filtered table name, fetch the data, and append it to the combined DataFrame
for table_name in filtered_table_names:
    print(f"Fetching data from table: {table_name}")
    query = f"SELECT * FROM {database_name}.{table_name}"
    df = pd.read_sql(query, conn)
    combined_df = pd.concat([combined_df, df], ignore_index=True)

# Save the combined DataFrame to a single Parquet file
combined_parquet_file_name = "combined_data.parquet"
combined_df.to_parquet(combined_parquet_file_name, engine='pyarrow')
print(f"All data exported to {combined_parquet_file_name}")







union_queries = []

for table_name, cols in table_columns.items():
    # Construct select clause with NULL for missing columns
    select_clause = ', '.join([f'"{col}"' if col in cols else f'NULL AS "{col}"' for col in all_columns])
    union_query = f"SELECT {select_clause} FROM {database_name}.{table_name}"
    union_queries.append(union_query)

combined_union_query = " UNION ALL ".join(union_queries)




# Assuming 'filtered_table_names', 'database_name', and 'conn' are already defined

table_columns = {}  # Dictionary to hold table names as keys and their columns as values

for table_name in filtered_table_names:
    query = f"DESCRIBE {database_name}.{table_name}"
    try:
        df = pd.read_sql(query, conn)
        # The column names might be in the 'column_name' column depending on the query response format
        # Adjust the column identifier as per your database's DESCRIBE output
        columns = df['column_name'].tolist() if 'column_name' in df else df['Column'].tolist()
        table_columns[table_name] = columns
    except Exception as e:
        print(f"Error fetching columns for table {table_name}: {e}")

# Now 'table_columns' dictionary will have all the table names as keys and their columns as values








def fetch_columns_for_table(table_name, conn):
    query = f"DESCRIBE {database_name}.{table_name}"
    try:
        df = pd.read_sql(query, conn)
        # The column names might be in the 'Column' column depending on the query response format
        columns = df['column_name'].tolist() if 'column_name' in df else df['Column'].tolist()
        return columns
    except Exception as e:
        print(f"Error fetching columns for table {table_name}: {e}")
        return []

table_columns = {table_name: fetch_columns_for_table(table_name, conn) for table_name in filtered_table_names}



common_columns = set.intersection(*map(set, table_columns.values()))
all_columns = set.union(*map(set, table_columns.values()))




from pyathena import connect
import pandas as pd

# Connect to Athena
conn = connect(aws_access_key_id='YOUR_ACCESS_KEY',
               aws_secret_access_key='YOUR_SECRET_KEY',
               s3_staging_dir='s3://YOUR_S3_BUCKET/path/to/',
               region_name='YOUR_AWS_REGION')

# Define the schema
schema = 'your_schema_name'

# Query to get all tables in the schema
query = f"SHOW TABLES IN {schema}"
tables = pd.read_sql(query, conn)

# List to hold tables containing the desired columns
tables_with_column = []

# Loop through tables to check for column names starting with 'xyzaa'
for table in tables['tab_name']:  # Ensure the column name 'tab_name' is correct based on output
    try:
        # Query to get the schema (column names) of the table
        col_query = f"SHOW COLUMNS IN {schema}.{table}"
        columns = pd.read_sql(col_query, conn)
        
        # Check for columns starting with 'xyzaa'
        filtered_columns = columns[columns['field'].str.startswith('xyzaa')]
        
        if not filtered_columns.empty:
            tables_with_column.append((table, filtered_columns['field'].tolist()))
    except Exception as e:
        print(f"Error processing table {table}: {str(e)}")

# Print tables that contain columns starting with 'xyzaa'
for table, cols in tables_with_column:
    print(f"Table '{table}' contains columns: {cols}")









