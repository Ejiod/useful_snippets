import os
import pandas as pd


# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.xlsx'):  # Check if the file is an Excel file
        file_path = os.path.join(folder_path, filename)
        xls = pd.ExcelFile(file_path)
        sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
        
        for sheet_name in sheet_names:
            df = pd.read_excel(file_path, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
            df['SheetName'] = sheet_name  # Add a new column for the sheet name
            all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed
combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)



(?:(?!not\s+sent|not\s+issued))

import glob
import pandas as pd

# Specify the folder where your Excel files are located
folder_path = 'your_folder_path_here'

# Initialize an empty list to store dataframes for each sheet
all_dataframes = []

# Use glob to get a list of Excel files in the folder
excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))

# Iterate through the Excel files
for excel_file in excel_files:
    xls = pd.ExcelFile(excel_file)
    sheet_names = xls.sheet_names  # Get the names of all sheets in the Excel file
    
    for sheet_name in sheet_names:
        df = pd.read_excel(excel_file, sheet_name=sheet_name)  # Read the Excel sheet into a DataFrame
        df['SheetName'] = sheet_name  # Add a new column for the sheet name
        all_dataframes.append(df)

# Combine all dataframes into one
combined_data = pd.concat(all_dataframes, ignore_index=True)

# Save the combined data to a new Excel file or do further processing as needed

combined_data.to_excel('combined_data_with_sheet_names.xlsx', index=False)







import pandas as pd
from spellchecker import SpellChecker

# Load your data
file_path = './Downloads/test_rev.csv'  # Make sure to use your actual file path
data = pd.read_csv(file_path)

# Initialize the spell checker
spell = SpellChecker()

# Function to identify and correct misspelled words in a text
def correct_spelling(text):
    if pd.isna(text):
        return text  # Return as is if the text is NaN
    misspelled = spell.unknown(text.split())
    corrected_text = text
    for word in misspelled:
        # Get the most likely correction
        correction = spell.correction(word)
        # Ensure both word and correction are valid strings
        if isinstance(word, str) and isinstance(correction, str) and word and correction:
            # Replace the misspelled word with the correction
            corrected_text = corrected_text.replace(word, correction)
    return corrected_text

# Applying spell checking to the 'Topic' and 'Content' columns
data['Corrected Topic'] = data['Topic'].apply(correct_spelling)
data['Corrected Content'] = data['Content'].apply(correct_spelling)

# Save or display the results
data.to_csv('data_spell2.csv')  # Replace with your desired output file path





ERROR LOGS
import pandas as pd
from sqlalchemy import create_engine
import datetime
import getpass
import socket
import re


    except Exception as e:
        finish_time = datetime.now()
        error_message = str(e)
        session_match = re.search(r'\[Session (\d+)\]', error_message)
        error_code_match = re.search(r'\[Error (\d+)\]', error_message)

        session_id = session_match.group(1) if session_match else 'N/A'
        error_code = error_code_match.group(1) if error_code_match else 'N/A'
        error_detail = 'Table already exists' if 'already exists' in error_message else 'Other error'

        consolidated_metadata.append({
            'user': getpass.getuser(),
            'host': socket.gethostname(),
            'source_db': row['source_db'],
            'source_table': row['source_table'],
            'target_db': row['target_db'],
            'target_table': row['target_table'],
            'error_session': session_id,
            'error_code': error_code,
            'error_message': error_detail,
            'time_executed': start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'time_finished': finish_time.strftime('%Y-%m-%d %H:%M:%S'),
            'database_server': 'target_host'  # Adjust as needed
        })











----SF---SP
CREATE OR REPLACE FUNCTION process_info_udf(input_data VARIANT)
RETURNS TABLE (col1 VARIANT, col5 VARIANT, col7 VARIANT, matches_regex BOOLEAN)
LANGUAGE PYTHON
RUNTIME_VERSION = 3.8
PACKAGES = ('pandas',)
HANDLER = 'udf'
AS $$
import pandas as pd

def udf(input_data):
    # Convert input VARIANT data to Pandas DataFrame
    df = pd.DataFrame(input_data)

    # Define the regex pattern
    regex_pattern = (
     Regex pattern
    )

    # Apply regex to the 'info_column' (assuming 'info_column' is one of the columns in your data)
    df['matches_regex'] = df['info_column'].str.contains(regex_pattern, regex=True, na=False)

    # Select only the required columns
    df = df[['col1', 'col5', 'col7', 'matches_regex']]

    # Convert DataFrame to list of dictionaries
    return df.to_dict(orient='records')
$$;


SELECT *
FROM TABLE(process_info_udf(
    (SELECT OBJECT_CONSTRUCT(*) FROM GESR)
));



from snowflake.snowpark.functions import year, col

# Extract the year and get unique years
df_with_year = df.with_column("year", year(col("batch_month")))
unique_years = df_with_year.select("year").distinct().collect()

# Iterate over each year and write data for that year
for year_row in unique_years:
    year_value = year_row['year']
    batch_df = df_with_year.filter(col("year") == year_value)
    batch_df.write.mode("append").save_as_table("db2.schema2.new_table")





from snowflake.snowpark.functions import to_date, year, col

# Assuming df is your DataFrame and 'batch_month' is a string in 'YYYY-MM-DD' format
df_with_date = df.with_column("year", year(to_date(col("batch_month"), 'YYYY-MM-DD')))

unique_years = [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]

# Iterate over each year and write data for that year
for year_value in unique_years:
    batch_df = df_with_date.filter(col("year") == year_value)
    batch_df.write.mode("append").save_as_table("db2.schema2.new_table")










-----
**What I Will Do:**
1. Leverage my analytical skills to support team diversity, ensuring data-driven decisions are made to foster an inclusive work environment.
2. Continually develop my data proficiency through learning and development programs, sharing acquired knowledge with the team.
3. Recognize and nurture talent within the team by providing data insights that support personal and professional development.

**How I Will Do It:**
1. Employ data analytics to identify trends and insights that support an open culture and help the team bring their best to work.
2. Engage in regular knowledge exchange sessions to learn from peers and contribute my expertise to the team's continuous growth.
3. Initiate and participate in projects that demonstrate the practical application of data analytics, encouraging stakeholder curiosity and understanding of our data-driven approach.

**What I Will Measure:**
1. Completion rate of data analytics-focused courses by team members to ensure continuous skill enhancement.
2. Progress against defined performance goals and the implementation of Personal Development Plans (PDPs) with a focus on analytics skills.
3. The increase in data literacy and curiosity among stakeholders as evidenced by engagement with analytics-driven projects and initiatives.

