from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
import os

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Merge Parquet Files") \
    .getOrCreate()

# Directory containing Parquet files
input_dir = "/path/to/parquet/files/"

# List all Parquet files
parquet_files = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith(".parquet")]

# Chunk the files into groups of 10
chunked_files = [parquet_files[i:i+10] for i in range(0, len(parquet_files), 10)]

# Read and merge each chunk
merged_df = None
for i, chunk in enumerate(chunked_files):
    # Read the Parquet files
    chunk_df = spark.read.parquet(*chunk)
    
    # Add a column to identify the chunk
    chunk_df = chunk_df.withColumn("chunk_id", lit(i))
    
    # Union the chunk with the merged DataFrame
    if merged_df is None:
        merged_df = chunk_df
    else:
        merged_df = merged_df.union(chunk_df)

# Write the merged DataFrame back to Parquet
output_dir = "/path/to/output/merged.parquet"
merged_df.write.mode("overwrite").parquet(output_dir)

# Stop SparkSession
spark.stop()
