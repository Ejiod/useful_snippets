import pandas as pd
import pyodbc

# Assuming you've already defined target_conn and tables_df

# Create the error log table
try:
    error_log_table_query = """
    CREATE TABLE error_log (
        source_table VARCHAR(255),
        target_table VARCHAR(255),
        error_code INTEGER,
        error_message VARCHAR(MAX),
        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """
    pd.read_sql(error_log_table_query, target_conn)
except Exception as e:
    print(f"Error creating error log table: {e}")

# Iterate over each row in the DataFrame for migration
for index, row in tables_df.iterrows():
    source_db = row['source_db']
    source_table = row['source_table']
    target_db = row['target_db']
    target_table = row.get('target_table', source_table)  # Assuming target table name is same as source if not provided

    # Create the target table by selecting data from the source table referenced via the foreign server
    try:
        query = f"CREATE TABLE {target_db}.{target_table} AS SELECT * FROM {source_db}.{source_table}@FServer"
        pd.read_sql(query, target_conn)
        print(f"Table {source_table} from {source_db} successfully migrated to {target_table} in {target_db}.")
    except pyodbc.Error as e:
        error_code = e.args[0]
        error_msg = str(e)
        print(f"Error migrating table {source_table} from {source_db} to {target_table} in {target_db}. Error Code: {error_code}, Error Message: {error_msg}")
        
        # Insert error details into the error log table
        insert_error_query = f"""
        INSERT INTO error_log (source_table, target_table, error_code, error_message)
        VALUES ('{source_table}', '{target_table}', {error_code}, '{error_msg}')
        """
        pd.read_sql(insert_error_query, target_conn)

# Close the target connection
target_conn.close()






CHECK THIS

import pyodbc
import pandas as pd
import datetime

# Connection strings for your databases (Replace with your actual connection details)
conn_str_db1 = 'DRIVER={Teradata};DBCNAME=your_host1;UID=your_user;PWD=your_pass;DATABASE=your_db1;'
conn_str_db2 = 'DRIVER={Teradata};DBCNAME=your_host2;UID=your_user;PWD=your_pass;DATABASE=your_db2;'

# Load the data with source and target information
data = pd.read_excel("path_to_your_file.xlsx")

logs = []

for index, row in data.iterrows():
    source_db = row['source_db']
    source_col_count = row['source_col_count']
    source_row_count = row['source_row_count']
    target_db = row['target_db']
    target_col_count = row['target_col_count']
    target_row_count = row['target_row_count']

    # Connect to source and target databases
    with pyodbc.connect(conn_str_db1) as conn1, pyodbc.connect(conn_str_db2) as conn2:
        source_query = f"SELECT COUNT(*) FROM {source_db}.{row['source_table']}"
        target_query = f"SELECT COUNT(*) FROM {target_db}.{row['target_table']}"

        source_result = pd.read_sql(source_query, conn1)
        target_result = pd.read_sql(target_query, conn2)

    # Validate if row and column counts are equal
    row_count_match = source_row_count == target_row_count
    col_count_match = source_col_count == target_col_count

    # Log details
    status = 'Success' if row_count_match and col_count_match else 'Failed'
    logs.append({
        'source_db': source_db,
        'source_col_count': source_col_count,
        'source_row_count': source_row_count,
        'target_db': target_db,
        'target_col_count': target_col_count,
        'target_row_count': target_row_count,
        'status': status,
        'validation_time': datetime.datetime.now()
    })

    # If both row and column counts match, drop the source table
    if row_count_match and col_count_match:
        with pyodbc.connect(conn_str_db1) as conn1:
            try:
                conn1.execute(f"DROP TABLE {source_db}.{row['source_table']}")
                logs[-1]['source_table_status'] = 'Dropped'
            except Exception as e:
                logs[-1]['source_table_status'] = 'Drop Failed'
                logs[-1]['drop_error_message'] = str(e)

# Convert logs to DataFrame
logs_df = pd.DataFrame(logs)
print(logs_df)

# Optionally, save logs to an Excel file
logs_df.to_excel("validation_logs.xlsx", index=False)

with pyodbc.connect(conn_str_db1) as conn1:
    for log in logs:
        placeholders = ', '.join(['?'] * len(log))
        columns = ', '.join(log.keys())
        # Include the database name in the INSERT statement if necessary
        sql = f"INSERT INTO {your_validation_db_name}.ValidationLog ({columns}) VALUES ({placeholders})"
        conn1.execute(sql, tuple(log.values()))
    conn1.commit()







READ PARQUET
import pandas as pd
import os

# Specify the directory path where your Parquet files are located
directory_path = '/path/to/your/directory'

# Get a list of all Parquet files in the directory
parquet_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.parquet')]

# Initialize an empty DataFrame to store the combined data
combined_data = pd.DataFrame()

# Loop through the Parquet files and read them into the combined DataFrame
for file in parquet_files:
    data = pd.read_parquet(file)
    combined_data = combined_data.append(data, ignore_index=True)

# Now, combined_data contains the data from all Parquet files in the directory

