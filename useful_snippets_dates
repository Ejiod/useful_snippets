import pandas as pd
import glob
import os

# Directory where your Parquet files are stored
directory_path = 'path/to/your/parquet/files/'

# Get a list of all Parquet files in the directory
parquet_files = sorted(glob.glob(f'{directory_path}/*.parquet'))

# Define a function to process a chunk of files
def process_chunk(file_chunk, output_filename):
    # Read and combine Parquet files in the chunk
    combined_df = pd.concat([pd.read_parquet(file) for file in file_chunk], ignore_index=True)
    
    # Export the combined DataFrame to a new Parquet file
    combined_df.to_parquet(output_filename)

# Number of files to process in each chunk
chunk_size = 700

# Create output directory if it doesn't exist
output_directory = 'path/to/output/files/'
os.makedirs(output_directory, exist_ok=True)

# Process files in chunks
for i in range(0, len(parquet_files), chunk_size):
    file_chunk = parquet_files[i:i + chunk_size]
    output_filename = os.path.join(output_directory, f'combined_{i//chunk_size + 1}.parquet')
    process_chunk(file_chunk, output_filename)
    print(f'Processed chunk {i//chunk_size + 1}: {len(file_chunk)} files')

print('All chunks processed.')
