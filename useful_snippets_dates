from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
import os

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Merge Parquet Files") \
    .getOrCreate()

# Directory containing Parquet files
input_dir = "/path/to/parquet/files/"

# List all Parquet files
parquet_files = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith(".parquet")]

# Chunk the files into groups of 10
chunked_files = [parquet_files[i:i+10] for i in range(0, len(parquet_files), 10)]

# Read and merge each chunk
merged_df = None
for i, chunk in enumerate(chunked_files):
    # Read the Parquet files
    chunk_df = spark.read.parquet(*chunk)
    
    # Add a column to identify the chunk
    chunk_df = chunk_df.withColumn("chunk_id", lit(i))
    
    # Union the chunk with the merged DataFrame
    if merged_df is None:
        merged_df = chunk_df
    else:
        merged_df = merged_df.union(chunk_df)

# Write the merged DataFrame back to Parquet
output_dir = "/path/to/output/merged.parquet"
merged_df.write.mode("overwrite").parquet(output_dir)

# Stop SparkSession
spark.stop()






from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("Read Oracle with Spark") \
    .config("spark.jars", "/path/to/ojdbc8.jar") \
    .getOrCreate()

# Oracle connection properties
oracle_connection_url = "jdbc:oracle:thin:@//host:port/service_name"
oracle_properties = {
    "user": "your_username",
    "password": "your_password",
    "driver": "oracle.jdbc.OracleDriver"
}

# Read table from Oracle database
df = spark.read.jdbc(url=oracle_connection_url, table="your_table_name", properties=oracle_properties)

# Show the data
df.show()

# Stop Spark Session
spark.stop()








import connectorx as cx
import pandas as pd

# Oracle connection string
connection_string = "oracle://username:password@host:port/service_name"

# SQL query
query = "SELECT * FROM your_table_name"

# Number of rows per chunk
chunk_size = 10_000_000

# Function to read and save data in chunks
def read_and_save_in_chunks(connection_string, query, chunk_size):
    offset = 0
    chunk_number = 0
    
    while True:
        # Modify the query to fetch a chunk of data
        chunk_query = f"{query} OFFSET {offset} ROWS FETCH NEXT {chunk_size} ROWS ONLY"
        
        # Read chunk into pandas DataFrame
        df = cx.read_sql(connection_string, chunk_query)
        
        # If no more data, break the loop
        if df.empty:
            break
        
        # Save the DataFrame to a Parquet file
        file_name = f"chunk_{chunk_number}.parquet"
        df.to_parquet(file_name, index=False)
        print(f"Saved {file_name}")
        
        # Update offset and chunk number for next iteration
        offset += chunk_size
        chunk_number += 1

# Call the function to start reading and saving data in chunks
read_and_save_in_chunks(connection_string, query, chunk_size)







import connectorx as cx
import polars as pl

# Oracle connection string
connection_string = "oracle://username:password@host:port/service_name"

# SQL query
query = "SELECT * FROM your_table_name"

# Number of rows per chunk
chunk_size = 10_000_000

# Function to read and save data in chunks
def read_and_save_in_chunks(connection_string, query, chunk_size):
    offset = 0
    chunk_number = 0
    
    while True:
        # Modify the query to fetch a chunk of data
        chunk_query = f"{query} OFFSET {offset} ROWS FETCH NEXT {chunk_size} ROWS ONLY"
        
        # Read chunk into Polars DataFrame
        df = pl.DataFrame(cx.read_sql(connection_string, chunk_query))
        
        # If no more data, break the loop
        if df.is_empty():
            break
        
        # Save the DataFrame to a Parquet file
        file_name = f"chunk_{chunk_number}.parquet"
        df.write_parquet(file_name)
        print(f"Saved {file_name}")
        
        # Update offset and chunk number for next iteration
        offset += chunk_size
        chunk_number += 1

# Call the function to start reading and saving data in chunks
read_and_save_in_chunks(connection_string, query, chunk_size)






import connectorx as cx
import polars as pl
import snowflake.connector
from snowflake.connector.pandas_tools import write_pandas

# Oracle connection string
connection_string = "oracle://username:password@host:port/service_name"

# SQL query
query = "SELECT * FROM your_table_name"

# Snowflake connection parameters
sf_params = {
    'user': 'your_sf_username',
    'password': 'your_sf_password',
    'account': 'your_sf_account',
    'warehouse': 'your_sf_warehouse',
    'database': 'your_sf_database',
    'schema': 'your_sf_schema'
}

# Number of rows per chunk
chunk_size = 10_000_000

# Function to read and load data in chunks
def read_and_load_in_chunks(connection_string, query, chunk_size, sf_params):
    offset = 0
    chunk_number = 0
    
    # Connect to Snowflake
    conn = snowflake.connector.connect(**sf_params)
    
    while True:
        # Modify the query to fetch a chunk of data
        chunk_query = f"{query} OFFSET {offset} ROWS FETCH NEXT {chunk_size} ROWS ONLY"
        
        # Read chunk into Polars DataFrame
        df = pl.DataFrame(cx.read_sql(connection_string, chunk_query))
        
        # If no more data, break the loop
        if df.is_empty():
            break
        
        # Convert Polars DataFrame to Pandas DataFrame
        pandas_df = df.to_pandas()
        
        # Load the DataFrame to Snowflake
        success, nchunks, nrows, _ = write_pandas(conn, pandas_df, 'your_sf_table')
        if success:
            print(f"Loaded {nrows} rows into Snowflake table (chunk {chunk_number})")
        else:
            print(f"Failed to load chunk {chunk_number} into Snowflake")
        
        # Update offset and chunk number for next iteration
        offset += chunk_size
        chunk_number += 1
    
    # Close Snowflake connection
    conn.close()

# Call the function to start reading and loading data in chunks
read_and_load_in_chunks(connection_string, query, chunk_size, sf_params)

