import pandas as pd
import pyodbc

# Assuming you've already defined target_conn and tables_df

# Create the error log table
try:
    error_log_table_query = """
    CREATE TABLE error_log (
        source_table VARCHAR(255),
        target_table VARCHAR(255),
        error_code INTEGER,
        error_message VARCHAR(MAX),
        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """
    pd.read_sql(error_log_table_query, target_conn)
except Exception as e:
    print(f"Error creating error log table: {e}")

# Iterate over each row in the DataFrame for migration
for index, row in tables_df.iterrows():
    source_db = row['source_db']
    source_table = row['source_table']
    target_db = row['target_db']
    target_table = row.get('target_table', source_table)  # Assuming target table name is same as source if not provided

    # Create the target table by selecting data from the source table referenced via the foreign server
    try:
        query = f"CREATE TABLE {target_db}.{target_table} AS SELECT * FROM {source_db}.{source_table}@FServer"
        pd.read_sql(query, target_conn)
        print(f"Table {source_table} from {source_db} successfully migrated to {target_table} in {target_db}.")
    except pyodbc.Error as e:
        error_code = e.args[0]
        error_msg = str(e)
        print(f"Error migrating table {source_table} from {source_db} to {target_table} in {target_db}. Error Code: {error_code}, Error Message: {error_msg}")
        
        # Insert error details into the error log table
        insert_error_query = f"""
        INSERT INTO error_log (source_table, target_table, error_code, error_message)
        VALUES ('{source_table}', '{target_table}', {error_code}, '{error_msg}')
        """
        pd.read_sql(insert_error_query, target_conn)

# Close the target connection
target_conn.close()






CHECK THIS

import pyodbc
import pandas as pd
import datetime

# Connection strings for your databases (Replace with your actual connection details)
conn_str_db1 = 'DRIVER={Teradata};DBCNAME=your_host1;UID=your_user;PWD=your_pass;DATABASE=your_db1;'
conn_str_db2 = 'DRIVER={Teradata};DBCNAME=your_host2;UID=your_user;PWD=your_pass;DATABASE=your_db2;'

# Load the data with source and target information
data = pd.read_excel("path_to_your_file.xlsx")

logs = []

for index, row in data.iterrows():
    source_db = row['source_db']
    source_col_count = row['source_col_count']
    source_row_count = row['source_row_count']
    target_db = row['target_db']
    target_col_count = row['target_col_count']
    target_row_count = row['target_row_count']

    # Connect to source and target databases
    with pyodbc.connect(conn_str_db1) as conn1, pyodbc.connect(conn_str_db2) as conn2:
        source_query = f"SELECT COUNT(*) FROM {source_db}.{row['source_table']}"
        target_query = f"SELECT COUNT(*) FROM {target_db}.{row['target_table']}"

        source_result = pd.read_sql(source_query, conn1)
        target_result = pd.read_sql(target_query, conn2)

    # Validate if row and column counts are equal
    row_count_match = source_row_count == target_row_count
    col_count_match = source_col_count == target_col_count

    # Log details
    status = 'Success' if row_count_match and col_count_match else 'Failed'
    logs.append({
        'source_db': source_db,
        'source_col_count': source_col_count,
        'source_row_count': source_row_count,
        'target_db': target_db,
        'target_col_count': target_col_count,
        'target_row_count': target_row_count,
        'status': status,
        'validation_time': datetime.datetime.now()
    })

    # If both row and column counts match, drop the source table
    if row_count_match and col_count_match:
        with pyodbc.connect(conn_str_db1) as conn1:
            try:
                conn1.execute(f"DROP TABLE {source_db}.{row['source_table']}")
                logs[-1]['source_table_status'] = 'Dropped'
            except Exception as e:
                logs[-1]['source_table_status'] = 'Drop Failed'
                logs[-1]['drop_error_message'] = str(e)

# Convert logs to DataFrame
logs_df = pd.DataFrame(logs)
print(logs_df)

# Optionally, save logs to an Excel file
logs_df.to_excel("validation_logs.xlsx", index=False)

with pyodbc.connect(conn_str_db1) as conn1:
    for log in logs:
        placeholders = ', '.join(['?'] * len(log))
        columns = ', '.join(log.keys())
        # Include the database name in the INSERT statement if necessary
        sql = f"INSERT INTO {your_validation_db_name}.ValidationLog ({columns}) VALUES ({placeholders})"
        conn1.execute(sql, tuple(log.values()))
    conn1.commit()







READ PARQUET
import pandas as pd
import os

# Specify the directory path where your Parquet files are located
directory_path = '/path/to/your/directory'

# Get a list of all Parquet files in the directory
parquet_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.parquet')]

# Initialize an empty DataFrame to store the combined data
combined_data = pd.DataFrame()

# Loop through the Parquet files and read them into the combined DataFrame
for file in parquet_files:
    data = pd.read_parquet(file)
    combined_data = combined_data.append(data, ignore_index=True)

# Now, combined_data contains the data from all Parquet files in the directory




---USING SQLALCHEMY

import pandas as pd
from sqlalchemy import create_engine, text
from datetime import datetime
import getpass
import socket

# Connection strings for SQLAlchemy engines
source_engine_str = 'teradatasql://username:password@source_host/source_db?authentication=LDAP'
target_engine_str = 'teradatasql://username:password@target_host/target_db?authentication=LDAP'

# Create SQLAlchemy engines for the source and target databases
source_engine = create_engine(source_engine_str)
target_engine = create_engine(target_engine_str)

# DataFrame containing the tables to migrate
df = pd.DataFrame({
    'source_table': ['TableA', 'TableB', 'TableC', 'TableD'],
    'target_table': ['TableA', 'TableB', 'TableC', 'TableD'],
    'source_db': ['SD', 'SD', 'SD', 'SD'],
    'target_db': ['TD', 'TD', 'TD', 'TD']
})

# Get the username and hostname for logging
username = getpass.getuser()
hostname = socket.gethostname()

# Prepare a list to collect error logs
error_logs = []

# Iterate over each row in the DataFrame and migrate tables
for index, row in df.iterrows():
    source_table_full = f"{row['source_db']}.{row['source_table']}"
    target_table_full = f"{row['target_db']}.{row['target_table']}"
    start_time = datetime.now()

    try:
        # Check if the target table already exists
        check_table_sql = text(f"SELECT TABLE_NAME FROM DBC.TablesV WHERE TableKind = 'T' AND TableName = '{target_table_full.split('.')[1]}' AND DatabaseName = '{target_table_full.split('.')[0]}'")
        table_exists = target_engine.execute(check_table_sql).fetchone()

        if table_exists:
            raise Exception(f"Table {target_table_full} already exists.")

        # Create the target table and copy the data from the source table
        create_table_sql = text(f"CREATE TABLE {target_table_full} AS (SELECT * FROM {source_table_full}) WITH DATA")
        target_engine.execute(create_table_sql)
        finish_time = datetime.now()

    except Exception as e:
        finish_time = datetime.now()
        error_logs.append({
            'user': username,
            'host': hostname,
            'source_db': row['source_db'],
            'source_table': row['source_table'],
            'target_db': row['target_db'],
            'target_table': row['target_table'],
            'error_code': e.__class__.__name__,
            'error_message': str(e),
            'time_executed': start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'time_finished': finish_time.strftime('%Y-%m-%d %H:%M:%S'),
            'database_server': 'target_host'  # or source_host depending on where the error occurred
        })

# Convert error logs to DataFrame and log them to a table
error_logs_df = pd.DataFrame(error_logs)
if not error_logs_df.empty:
    error_logs_df.to_sql(name='error_log_table', con=target_engine, if_exists='append', index=False)

# Dispose the SQLAlchemy engines
source_engine.dispose()
target_engine.dispose()


next

import pandas as pd
from sqlalchemy import create_engine

# Replace with your actual SQLAlchemy engine instances
source_engine = create_engine('source_connection_string')
target_engine = create_engine('target_connection_string')

# Function to fetch row count and size for a specific database and table
def fetch_metadata(engine, db_name, table_name):
    row_count_sql = f"SELECT COUNT(*) AS row_count FROM {db_name}.{table_name}"
    column_count_sql = f"SELECT COUNT(*) AS column_count FROM DBC.ColumnsV WHERE DatabaseName = '{db_name}' AND TableName = '{table_name}'"
    size_sql = f"SELECT SUM(CurrentPerm) AS table_size FROM DBC.TableSizeV WHERE DatabaseName = '{db_name}' AND TableName = '{table_name}' GROUP BY DatabaseName, TableName"

    # Fetch row count
    row_count_df = pd.read_sql(row_count_sql, engine)
    # Fetch column count
    column_count_df = pd.read_sql(column_count_sql, engine)
    # Fetch size
    size_df = pd.read_sql(size_sql, engine)

    return row_count_df.iloc[0]['row_count'], column_count_df.iloc[0]['column_count'], size_df.iloc[0]['table_size'] if not size_df.empty else 0

# DataFrame containing the tables to get metadata for
df = pd.DataFrame({
    'source_table': ['TableA', 'TableB', 'TableC', 'TableD'],
    'target_table': ['TableA', 'TableB', 'TableC', 'TableD'],
    'source_db': ['SD', 'SD', 'SD', 'SD'],
    'target_db': ['TD', 'TD', 'TD', 'TD']
})

# List to store consolidated metadata
consolidated_metadata = []

for index, row in df.iterrows():
    # Fetch metadata for source table
    source_row_count, source_column_count, source_table_size = fetch_metadata(source_engine, row['source_db'], row['source_table'])
    
    # Fetch metadata for target table
    target_row_count, target_column_count, target_table_size = fetch_metadata(target_engine, row['target_db'], row['target_table'])

    # Add to consolidated list
    consolidated_metadata.append({
        'source_db': row['source_db'],
        'target_db': row['target_db'],
        'SOURCE_row_count': source_row_count,
        'SOURCE_column_count': source_column_count,
        'TARGET_row_count': target_row_count,
        'TARGET_column_count': target_column_count,
        'source_table': row['source_table'],
        'target_table': row['target_table'],
        'source_table_byte': source_table_size,
        'target_table_byte': target_table_size
    })

# Convert the list to a DataFrame
metadata_df = pd.DataFrame(consolidated_metadata)

# Display or save the metadata
print(metadata_df)
metadata_df.to_excel("tables_metadata.xlsx", index=False)




To SF

import snowflake.connector
from snowflake.connector.pandas_tools import write_pandas

# ... (other setup code)

# Connect to Snowflake
snowflake_ctx = snowflake.connector.connect(**snowflake_conn_params)
cs = snowflake_ctx.cursor()

# Example loop over the DataFrame rows
for index, row in table_list_df.iterrows():
    source_table = row['source_table']
    source_db = row['source_db']
    target_table = row['target_table']
    
    # Connect to Teradata and fetch the data
    with teradatasql.connect(**teradata_conn_params) as teradata_conn:
        teradata_query = f"SELECT * FROM {source_db}.{source_table}"
        df = pd.read_sql(teradata_query, teradata_conn)
    
    # Write the data to Snowflake, automatically creating the table if it doesn't exist
    write_pandas(snowflake_ctx, df, target_table, auto_create_table=True)

# Close the Snowflake connection
cs.close()
snowflake_ctx.close()











import pandas as pd
import cx_Oracle

# Function to split list into chunks of max size 1000
def chunker(seq, size):
    return (seq[pos:pos + size] for pos in range(0, len(seq), size))

# Assuming df is your DataFrame and it has a column named 'Acc'
acc_list = df['Acc'].tolist()

# Database connection
connection_string = 'username/password@hostname:port/service_name'
connection = cx_Oracle.connect(connection_string)

# Empty DataFrame to store results
result_df = pd.DataFrame()

# Process in chunks
for chunk in chunker(acc_list, 1000):
    acc_values = ','.join([f"'{acc}'" for acc in chunk])
    query = f"SELECT * FROM your_table WHERE Acc IN ({acc_values})"
    chunk_df = pd.read_sql(query, con=connection)
    result_df = pd.concat([result_df, chunk_df], ignore_index=True)

# Close the connection
connection.close()


File operation
import os
import msoffcrypto
import io
import pandas as pd
import shutil  # For file operations like move

# Directory containing the Excel files
directory = "path/to/excel/files"

# Password for encryption
password = "yourpassword"

for filename in os.listdir(directory):
    if filename.endswith(".xlsx"):
        original_filepath = os.path.join(directory, filename)
        
        # Load the Excel file into a DataFrame
        df = pd.read_excel(original_filepath)
        
        # Create an in-memory output file for the encrypted version
        output = io.BytesIO()
        df.to_excel(output, index=False)
        output.seek(0)
        
        # Temporary encrypted file path
        temp_encrypted_filepath = os.path.join(directory, f"temp_encrypted_{filename}")
        
        # Encrypt the file and save it as a temporary file
        with open(temp_encrypted_filepath, "wb") as protected_file:
            office_file = msoffcrypto.OfficeFile(output)
            office_file.load_key(password=password)
            office_file.encrypt(protected_file)
        
        # Replace the original file with the encrypted one
        shutil.move(temp_encrypted_filepath, original_filepath)
        
        print(f"Encrypted and replaced {filename}")



from msoffcrypto.format.ooxml import OOXMLFile
import os

# Directory containing the Excel files
directory = "path/to/excel/files"

# Password for encryption
password = "yourpassword"

for filename in os.listdir(directory):
    if filename.endswith(".xlsx"):
        file_path = os.path.join(directory, filename)
        
        # Open the current Excel file
        plain = open(file_path, "rb")
        file = OOXMLFile(plain)
        
        # Temporary encrypted file path
        temp_encrypted_filepath = os.path.join(directory, f"temp_encrypted_{filename}")
        
        # Encrypt and save the file
        with open(temp_encrypted_filepath, "wb") as encrypted_file:
            file.encrypt(password, encrypted_file)
        
        # Close the original file
        plain.close()
        
        # Replace the original file with the encrypted one
        os.remove(file_path)  # Delete the original file
        os.rename(temp_encrypted_filepath, file_path)  # Rename the encrypted file to the original file name
        
        print(f"Encrypted and replaced {filename}")



= ((A2^2 * A4 * (1 - A4)) / (A3^2)) / (1 + ((A2^2 * A4 * (1 - A4)) / (A3^2 * A5)))
=NORM.S.INV(1-(1-0.999)/2)


def clean(df):
    
    address_columns = ['']

    # Function to shift non-empty values to the left in each row
    def shift_values(row):
        filtered_values = [value for value in row if value not in ('', None)]
        num_empty_values = len(row) - len(filtered_values)
        return filtered_values + [''] * num_empty_values

    df[address_columns] = df[address_columns].apply(lambda row: shift_values(row), axis=1, result_type='expand')
    df[address_columns].columns = address_columns

    return df




